<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <title>Markov Decision Processes</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel --> 
  <link rel="stylesheet" href="http://capfab.io/plugins/slick/slick.css" />
  <link rel="stylesheet" href="http://capfab.io/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="http://capfab.io/plugins/font-awesome/css/font-awesome.min.css" />
  <!-- Bootstrap -->
  <link rel="stylesheet" href="http://capfab.io/plugins/bootstrap/bootstrap.min.css" />
  <link rel="stylesheet" href="http://capfab.io/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="http://capfab.io/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="http://capfab.io/images/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="http://capfab.io/images/favicon.png" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-179028456-1"></script>
  <script data-ad-client="ca-pub-3914604134466918" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-179028456-1');
  </script>
	
</head>

  <body>
    <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
      <a href="http://capfab.io/" class="navbar-brand">
          <img src="http://capfab.io/images/site-navigation/logo.png" alt="site-logo">
      </a>
      <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
          <span class="navbar-toggler-icon"></span>
          <span class="navbar-toggler-icon"></span>
          <span class="navbar-toggler-icon"></span>
      </button>
  
      <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
          <ul class="nav navbar-nav main-navigation">
              
              
                <li class="nav-item">
                  <a href="http://capfab.io/#home" class="nav-link ">Home</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#about" class="nav-link ">About</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#blog" class="nav-link ">Blog</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#contact" class="nav-link ">Contact</a>
                </li>
              
          </ul>
          <div class="navbar-nav">
              <a href="http://capfab.io/contact" class="hire_button">Let's Talk</a>
          </div>
      </div>
  </div>
</nav>

    <div id="content">
      

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Markov Decision Processes</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          <ol class="breadcrumb align-items-center">
            <li class="breadcrumb-item"><a href=http://capfab.io/>Home</a></li>
            <li class="breadcrumb-item"><a href=http://capfab.io/blog>All Post</a></li>
            <li class="breadcrumb-item active" aria-current="page">Markov Decision Processes</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
      <img src=http://capfab.io/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
      
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
          <defs>
              <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
                  <stop offset="0" stop-color="#f1f6f9" />
                  <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
              </linearGradient>
          </defs>
          <g data-name="blob-shape (3)">
              <path class="blob" fill="url(#d)"
                  d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
          </g>
      </svg>
  </div>
  <div class="animate-pattern">
      <img src=http://capfab.io/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
      <div class="row">
          <div class="col-lg-12">
              <div class="singleBlog__feature">
                  <img src=http://capfab.io/images/single-blog/mdp_pic.jpg alt="feature-image">
              </div>
          </div>
      </div>
 
      <div class="row mt-5">
          <div class="col-lg-12">
            <div class="singleBlog__content">
	      <h2>
		Table of Contents
	      </h2>
	      <aside>
		<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#what-are-markov-processes-">What are Markov Processes ?</a>
      <ul>
        <li><a href="#markov-property">Markov property</a></li>
      </ul>
    </li>
    <li><a href="#rewards">Rewards</a></li>
    <li><a href="#value-function">Value Function</a></li>
    <li><a href="#bellman-equation">Bellman Equation</a></li>
    <li><a href="#the-action-space">The Action Space</a></li>
    <li><a href="#the-policy">The Policy</a></li>
    <li><a href="#the-optimal-policy">The Optimal Policy</a></li>
    <li><a href="#bellman-optimality-equation">Bellman Optimality Equation</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
	      </aside>

                <h2 id="introduction">Introduction</h2>
<p>In this blog post, we are going to talk about Markov Decision Processes (MDP).
The format is a bit longer than the usual blog post. We will go into quite a bit of
details about MDPs. Some mathematics will be involved.</p>
<p>MDPs are very important in the context of reinforcement learning (RL), because lots
of RL problems can be put into some form or another as MDPs. For example, the bandit
problem, which is a typical RL problem, is actually an MDP with one state. One can
also have MDPs with a full observable environment (e.g. chess game) or with partially
observable environment (financial market).</p>
<h2 id="what-are-markov-processes-">What are Markov Processes ?</h2>
<p>So, what is a Markov Process anyway? To understand that, we first need to clarify
what does Markov means.</p>
<h3 id="markov-property">Markov property</h3>
<p>If you actually take a random variable $S$, then the Markov
property tells us that the future values $S_{t+j}$ of that variable are independent of
of the past values $S_{t-i}$, knowing (or conditioned) on the present value $S_t$.
This is a pretty interesting property, because it means that all of the history that
happened to that variable is actually irrelevant to the future values of the variable.
Imagine a chess game: two of your friends start a game and you let them play, while you
cook some delicious meal in the kitchen. Then, one of your friends is kind of tired and
does not want to play anymore, but the other is not very happy because he felt he could
win the game, and he would like to continue. So, you propose to continue to play,
replacing your friend. Should you be aware of all the moves that were done? No. The
game is fully characterized by the positions of the pieces at that particular moment.
All that really matters for the future of the game is already on the board.</p>
<p>Mathematically speaking, if we consider a state $S_t$ (a state in general means
&ldquo;information available at a particular instant $t$&rdquo;, in our case the random variable
or the chess board with positioning of the pieces), then the probability distribution
of the state $S_{t+1}$ only depends on $S_t$, i.e.</p>
<p>$$
\mathbb{P}\left[S_{t+1} \mid S_t, S_{t-1},\cdots, S_1, S_0 \right] = \mathbb{P}\left[S_{t+1} \mid S_t \right]
$$</p>
<p>Another way to understand this is to basically consider that you start at a state $s$
and you have the next state $s'$, then the state transition probability is basically
defined by:</p>
<p>$$
\mathbb{P}\left[S_{t+1}=s&rsquo; \mid S_t =s \right] = \mathcal{P}_{ss&rsquo;}.
$$</p>
<p>Therefore, I can transition
to the next state that is completely characterized by the present state. Here, we have
that $S_{t+1}$ is a particular instantiation of $s'$, while $S_{t}$ corresponds to
the state $s$.</p>
<p>Once we have that state transition probability $\mathcal{P}_{ss&rsquo;}$, we can represent
a state transition matrix $\mathcal{P}$, where the index  of my rows represent the present
state $s$ where my system is and the index of the columns represents the potential next
state where my system might transition to. Therefore, the matrix $\mathcal{P}$ is
represented by:</p>
<p>$$
\begin{equation*}
\mathcal{P} =
\begin{bmatrix}
\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \\<br>
\vdots  &amp; \ddots &amp; \vdots  \\<br>
\mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}
\end{bmatrix}
\end{equation*}
$$</p>
<p>This matrix provides the full information about how the Markov process evolves.
We can sample from it and it will provide different kinds of possible evolutions
to my system.</p>
<p>We are finally equipped to define an MDP: it is a sequence of finite states that are
fully characterized by the transition probability matrix $\mathcal{P}$. Therefore,
an MDP can be fully defined by a state space $\mathcal{S}$ and a transition
probability $\mathcal{P}$.</p>
<h2 id="rewards">Rewards</h2>
<p>Now that we have defined what a Markov process is, we will dig into the decision part of
the MDPs. To be able to take <em>good</em> (whatever that might mean) decisions, we will need to
introduce a value judgement that is called the <strong>reward</strong> in the RL framework. Such reward
represents what the <em>agent</em> gets when it transitions from the state $s$ to the state $s'$.</p>
<p>We have now a <em>Markov reward process</em> that is defined through the tuple $(\mathcal{S},\mathcal{P},
\mathcal{R},\gamma)$, where $\mathcal{R}$ is defined as a reward function that tells us how
much reward we get from the state $s$, i.e.</p>
<p>$$
\mathcal{R}_s = \mathbb{E} \left[R_t \mid S_t = s \right]
$$</p>
<p>and $\gamma \in \left[0,1\right]$ is a discount factor that considers what's the importance
that we provide to rewards far in the future versus immediate rewards. If we do have $\gamma = 1$,
then we care about all the rewards far into the future, while in the case $\gamma=0$,
we care about immediate rewards only.</p>
<p>Based on that, we can introduce the basic goal in a reinforcement learning, which is
to maximize the return $G_t$ that corresponds to the total discounted reward that we sum up
over all the states through which the system is gonna pass through, i.e.</p>
<p>$$
G_t = R_t + \gamma R_{t+1} +\cdots  = \sum_{k=0}^\infty \gamma^k R_{t+k}
$$</p>
<p>If we do consider that there is indeed an infinite amount of steps, we do see that the discount
factor $\gamma$ plays as well a very useful mathematical role, which is to make the series
finite. There is however undiscounted Markov reward processes with $\gamma=1$. We will not dig into
it in here though.</p>
<h2 id="value-function">Value Function</h2>
<p>Until now, we did not talk about expectations, because we were considering the case of
a particular sample and its corresponding total reward. However, at the end of the day what we do
care about is expectations. In the case of MDPs, we talk about <strong>value function</strong> as providing
the long-term value of a state $s$. Therefore, the expected return from a state $s$ is</p>
<p>$$
v(s) = \mathbb{E}\left[G_t \mid S_t =s \right].
$$</p>
<p>Being in a non-deterministic environment, you don't know exactly what would be your final total
return $G_t$, but you can compute your expected return based on the transition probability
matrix.</p>
<h2 id="bellman-equation">Bellman Equation</h2>
<p>Now that we have defined the value function, we can finally introduce the most important relation
in all MDPs: the Bellman equation. The basic idea is to split the reward into two parts: the
immediate reward that you get and what comes after that immediate reward. Let's introduce the
definition of $G_t$ into the previous definition $v(s)$:</p>
<p>$$
\begin{eqnarray}
v(s) &amp;=&amp; \mathbb{E}\left[G_t \mid S_t = s \right] \nonumber \\<br>
&amp;=&amp; \mathbb{E}\left[R_t + \gamma R_{t+1}+ \gamma^2 R_{t+2}+ \cdots \mid S_t = s \right] \nonumber  \\<br>
&amp;=&amp; \mathbb{E} \left[R_t + \gamma \left(R_{t+1}+ \gamma R_{t+2}+ \cdots\right) \mid S_t = s \right] \nonumber  \\<br>
&amp;=&amp; \mathbb{E} \left[R_t + \gamma G_{t+1} \mid S_t = s \right] \nonumber  \\<br>
&amp;=&amp; \mathbb{E} \left[R_t + \gamma v(S_{t+1}) \mid S_t = s \right] \nonumber \quad \scriptstyle{\href{https://en.wikipedia.org/wiki/Law_of_total_expectation}{\text{; using the law of total expectation}}} \\<br>
\end{eqnarray}
$$</p>
<p>You do see a recurrent relation that tells us how good it is to be in a
particular state $s$ depends on the immediate reward plus how good it is to be in the next
state with a discounted factor $\gamma$.</p>
<p>Once we have that definition of the value function, we can rewrite it in terms of the transition
probability matrix $\mathcal{P}_{ss&rsquo;}$ and the reward function $\mathcal{R}_s$ at state $s$ by just
inserting the definitions of the expectations into the equation. That leads to the following
relation:</p>
<p>$$
\begin{equation}
v(s) = \mathcal{R}_{s} + \gamma \sum_{s&rsquo; \in \mathcal{S}} \mathcal{P}_{ss&rsquo;}v(s&rsquo;)
\end{equation}
$$</p>
<p>We can also rewrite that Bellman equation into something that is rather straigthforward to understand,
which are matrices and vectors:</p>
<p>$$
v = \mathcal{R}+\gamma \mathcal{P}v.
$$
Being a linear equation, we can then invert it, getting
$$
v = \left( 1 - \gamma \mathcal{P}\right)^{-1} \mathcal{R}.
$$</p>
<p>There are a bunch of methods that can be used to solve the Bellman equation for large MDPs, such as
linear programming, Temporal-Difference learning, etc. We might address some of those techniques in
future blog posts.</p>
<p>Let us now introduce the final essential element of MDPs: actions.</p>
<h2 id="the-action-space">The Action Space</h2>
<p>A MDP is basically a Markov reward process with decisions, therefore we can define the MDP through
the tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P},\mathcal{R} \rangle $, where $\mathcal{A}$
is a finite set of actions. Based on the existence of such set of actions, we are now able to
actually generalize all of the previous concepts introduced. In particular, we can say that the
state transition probability matrix depends not only on the state where you are, but as well on
the action that you take:</p>
<p>$$
\mathcal{P}_{ss&rsquo;}^{a} = \mathbb{P}\left[S_{t+1} =s&rsquo; \mid S_t =s, A_t = a \right].
$$</p>
<p>Therefore, the probability of ending up on all the possible different states depends on the action
that you take at the instant $t$ and at which state you are at time $t$. The reward function
$\mathcal{R}$ may as well depend on the action:</p>
<p>$$
\mathcal{R}^a_{s} = \mathbb{E}\left[R_{t+1} \mid S_t = s, A_t = a \right].
$$</p>
<p>Apart from that, everything is the same.</p>
<h2 id="the-policy">The Policy</h2>
<p>We are now well equipped to define what it means to make and take decisions. In order to do that
we need to define what is called a <strong>policy</strong>. The formal definition of a policy $\pi$ is a
distribution over actions given states:</p>
<p>$$
\pi(a | s) = \mathbb{P} \left[A_t = a \mid S_t = s \right].
$$</p>
<p>In other words, if our system is in a particular state $S_t$, what's the probability of taking
a particular action $A_t$. Therefore, once you have a policy you have fully defined the behaviour
of an agent taking action in a particular system (remember the chess player? that's our agent in
that example, i.e. the person taking actions).</p>
<p>Another interesting implication of the Markov property is that the policy only depends on
the current state (and not the past states) as we discussed  for the Chess example. Therefore, the
policy is said to be stationary or time-independent.</p>
<p>Also the policy depends on the rewards through the state where the system is, because the state where
the system is characterized by the immediate and expected future rewards of the agent.</p>
<p>What defines a Markov reward process given by a chain of states and rewards is the averaging over
policies of our transition probability matrix and reward function, i.e.</p>
<p>$$
\begin{eqnarray}
\mathcal{P}^{\pi}_{ss&rsquo;} &amp;=&amp; \sum_{a \in \mathcal{A}} \pi\left(a \mid s \right) \mathcal{P}^a_{ss&rsquo;} \nonumber \\<br>
\mathcal{R}^{\pi}_{s} &amp;=&amp; \sum_{a \in \mathcal{A}} \pi\left(a \mid s \right) \mathcal{R}^a_{s} \nonumber
\end{eqnarray}
$$
As such the Markov reward process corresponds to the tuple
$\langle \mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi} \rangle$.</p>
<p>Previously, the value function didn't have any agent, no way to define actions. We have now a
way to choose the value function through the policy $\pi$ and as such, the <em>state-value function</em>
$v_{\pi} (s)$ of an MDP becomes the expected return from state $s$ that follows a policy $\pi$</p>
<p>$$
v_{\color{red}{\pi}}(s) = \mathbb{E}_{\color{red}{\pi}}\left[G_t \mid S_t = s \right];
$$
we have an expectation $\mathbb{E}_{\pi}$ over the total return when we sample the actions following
the policy $\pi$.</p>
<p>We can also defined a second type of function that is the action value function, which tells us how good
it is to take a particular action from a particular state. This is the mathematical object that whe
should consider when we have to take a particular action. Therefore, the action value function is
the expected return from a state $s$, taking an action $a$ and by following a particular policy $\pi$:</p>
<p>$$
q_{\pi}(a,s) = \mathbb{E}_{\pi}\left[G_t \mid S_t = s, A_t = a \right]
$$</p>
<p>A new Bellman equation is obtained, as previously by decomposing the immedidate reward plus the
value of the next state:</p>
<p>$$
v_{\pi}(s) = \mathbb{E}_{\pi}\left[R_t +  \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right]
$$</p>
<p>In a similar way, we can get an equation for the action-value function</p>
<p>$$
q_{\pi}(s,a) = \mathbb{E}_{\pi}\left[R_t +  \gamma q_{\pi}(S_{t+1},A_{t+1}) \mid S_t = s \right].
$$</p>
<p>That last equation allows to relate the action-value of the next state with respect to the state
where my system is right now.</p>
<p>The way to understand it a bit better is through the relationship that is present between $v$ and $q$.
So, in order to get $v_{\pi}(s)$, we are actually averaging over all the possible actions that we might
take in the future. Since each action is really <em>valued</em> by the action-value function $q_{\pi}$ (at
the next state), then we need to average over all the action-values under a certain policy $\pi$ (since the
actions are actually sampled from a particular policy), providing us the value of the present state $s$, i.e.</p>
<p>$$
v_{\pi} (s) = \sum_{a \in \mathcal{A}} \pi(a\mid s) q_{\pi}(s,a).
$$</p>
<p>Let's now consider instead that we are going to take a particular action $a$ at a particular state $s$. If we take
the example of the chess game, we are now asking the question: how good is it to take a specific move, while in the
previous paragraph, we were asking how good is it to be where I am now in the game (basically, my probability of
winning the game). Therefore, we have to average over the possible states where the action that we taking are
going to lead me (plus the immediate reward), i.e.</p>
<p>$$
q_{\pi}(s,a) = \mathcal{R}^a_{s} + \gamma \sum_{s&rsquo; \in \mathcal{S}} \mathcal{P}^a_{ss&rsquo;}v_{\pi}(s&rsquo;).
$$</p>
<p>If we put the last two equations together, we end up with the following recursrive relation</p>
<p>$$
\begin{equation}
\label{eqn:vp_bellman}
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a\mid s)\left( \mathcal{R}^a_{s}+\gamma\sum_{s&rsquo;\in \mathcal{S}} \mathcal{P}^a_{ss&rsquo;} v_{\pi}(s&rsquo;) \right)
\end{equation}
$$</p>
<p>which is the new Bellman equation for $v_{\pi}$ that we were looking for. We can also do
the same trick with $q_{\pi}$ to end up with the following recursive relation:</p>
<p>$$
\begin{equation}
\label{eqn:qp_bellman}
q_{\pi}(s) =  \mathcal{R}^a_{s} + \gamma \sum_{s&rsquo; \in \mathcal{S}} \mathcal{P}^a_{ss&rsquo;} \sum_{a&rsquo; \in \mathcal{A}} \pi(a&rsquo;\mid s&rsquo;) q_{\pi}(s&rsquo;,a&rsquo;)
\end{equation}
$$</p>
<p>The equation $(\ref{eqn:vp_bellman})$ and $(\ref{eqn:qp_bellman})$ are
actually how we solve the MDPs. If you abstract the math, you understand that the
idea behind those two equations are really simple: the value function at the actual
step is just the immediate reward plus the value function at the step where you are
after taking a particular action.</p>
<p>That's all fine, but what we are looking for are actually the optimal actions to
pick. For that, we will need to get the optimal policy.</p>
<h2 id="the-optimal-policy">The Optimal Policy</h2>
<p>Given a state you are in, you want to pick actions that will provide you the
maximum future rewards. That policy is called the optimal policy.</p>
<p>Let's first define the <em>optimal state-value</em> function $v_{*}(s)$ as being the maximum
value function over all possible policies:</p>
<p>$$
v_{*}(s) = \max_{\pi} v_{\pi}(s).
$$</p>
<p>This function basically tells us what's the maximum possible reward we can extract from
the particular system we are in. In a similar fashion, one can define the <em>optimal
action-value</em> function $q_{*}(s,a)$ as being the maximum action-value function
over all policies</p>
<p>$$
q_{*}(s, a) = \max_{\pi}q_{\pi}(s, a),
$$</p>
<p>meaning if you commit to a particular action, what's the most reward you can get.
What's important to notice is that if we know $q_{*}$, then we have solved the MDP,
because under all policies it allows to understand the maximum reward you can get for
a particular action. Therefore, knowing $q_{*}$ allows us to behave optimally under the
MDP. As such, (and again) solving an MDP is actually getting $q_{*}$.</p>
<h2 id="bellman-optimality-equation">Bellman Optimality Equation</h2>
<p>Of course, that's all good. But in practice, how do we get $q_{*}$ anyways ? Well, you
need to actually get the <strong>Bellman optimality equation</strong> and solve it.
Before, we were looking at expectation on action and rewards. Now, we are really looking at the
maximum returns. So, our two previous equations $(\ref{eqn:vp_bellman})$ and $(\ref{eqn:qp_bellman})$
become:</p>
<p>$$
\begin{equation}
\label{eqn:vp_bellman_optimal}
v_{*}(s) = \max_a \mathcal{R}^a_{s}+\gamma\sum_{s&rsquo;\in \mathcal{S}} \mathcal{P}^a_{ss&rsquo;} v_{*}(s&rsquo;),
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{eqn:ap_bellman_optimal}
q_{*}(s,a) =  \mathcal{R}^a_{s}+\gamma\sum_{s&rsquo;\in \mathcal{S}} \mathcal{P}^a_{ss&rsquo;} \max_{a&rsquo;}q_{*}(s&rsquo;,a&rsquo;),
\end{equation}
$$</p>
<p>based on the fact that an optimal policy is found by maximizing over $q_{*}(s,a)$, i.e.</p>
<p>$$
\begin{equation}
\pi_{*}(a\mid s) =
\begin{cases}
1 &amp; \text{if } a= \text{arg}\max_{a\in\mathcal{A}}q_{*}(s,a), \nonumber \\<br>
0 &amp; \text{otherwise} \nonumber .
\end{cases}    <br>
\end{equation}
$$</p>
<p>Now that we have the Bellman optimality equation, we should be done. Unfortunately, things are not
that easy because the equations $(\ref{eqn:vp_bellman_optimal})$ and $(\ref{eqn:ap_bellman_optimal})$
are not linear anymore :sad: . Moreover, there is no closed form solution because there are some
<strong>max</strong> involved that complexifies the problem. So, we have to resort to iterative solution methods
such as Q-learning and dynamic programming methods. We will talk about those in a subsequent blog post.</p>
<h2 id="conclusion">Conclusion</h2>
<p>So, let's stop here. We have put in place the foundations for the understanding of a reinforcement learning
setup through the study of MDPs. We have defined the Markov property and the closely related process
of Markov Reward Processes. Then, after having introduced rewards, we talked about policies and the
actions that are sampled from those policies. We derived some (Bellman) equations that allow to
connect the (action)-value function at a certain state with the (action)-value functions at a future
state. Finally, we approached the subject of optimal policies and how to choose optimal actions by
solving the Bellman optimality equation.</p>
<hr>
<p>If you would like to cite the present content, please use:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">
 @article<span style="color:#f92672">{</span>fabio2021mdps,
   title   <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Markov Decision Processes&#34;</span>,
   author  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Capela, Fabio&#34;</span>,
   journal <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;capfab.io/blog&#34;</span>,
   year    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;2021&#34;</span>,
   url     <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://capfab.io/blog/markov-decision-processes&#34;</span>
 <span style="color:#f92672">}</span>

</code></pre></div><hr>
<h2 id="references">References</h2>
<p>[1] Reinforcement Learning: An Introduction, Adrew Barto, Richard Sutton</p>

		<center>
		<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="QZWRXr8" data-color="#5F7FFF" data-emoji="" data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#ffffff" data-coffee-color="#FFDD00" ></script>
		<center/>
		<br>
		<br>
		<form style="border:1px solid #ccc;padding:3px;text-align:center;" action="https://tinyletter.com/stacknets" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/stacknets', 'popupwindow', 'scrollbars=yes,width=700,height=600');return true"><p><label for="tlemail"><b>Join The Newsletter Club</b></label></p><p><input type="text" style="width:200px" name="email" id="tlemail" /></p><input type="hidden" value="1" name="embed"/><input type="submit" value="Sign me up" /><p></p></form>
		<br>
		<br>
		<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "capfabio" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
              </div>
          </div>
      </div>
  </div>
</section>


    </div>
    <section class="contact" id="contact">
  <div class="contact__background_shape">
      <svg viewBox="0 0 1920 79">
          <path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
      </svg>
  </div>
  <div class="container">
      <div class="row">
          <div class="col-lg-12">
              <div class="contact__cta">
                  <div class="shape-1">
                      <svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
                          <path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/>
                      </svg>
                  </div>
                  <div class="shape-2">
                      <svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
                          <path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/>
                      </svg>
                  </div>
                  <div class="contact__cta_content">
                      <span>Contact me</span>
                      <h1>Let's get in touch</h1>
                  </div>
                  <div class="contact__cta_action">
                      <a href="http://capfab.io/contact">Message Me</a>
                  </div>
              </div>
          </div>
      </div>
      <div class="row contact__widget">
          <div class="col-lg-4">
              <div class="contact__widget_logo">
                  <img src="http://capfab.io/images/contact/widget-logo.png" alt="widget-logo">
              </div>
          </div>
          <div class="col-lg-4">
              <div class="contact__widget_sitemap">
                  <h3>Sitemap</h3>
                  <ul>
                      
                      
                        <li><a href="http://capfab.io/blog">Blog</a></li>
                      
                        <li><a href="http://capfab.io/index.xml">RSS Feed</a></li>
                      
                        <li><a href="https://tinyletter.com/stacknets">Newsletter</a></li>
                      
                  </ul>
              </div>
          </div>
          <div class="col-lg-4">
              <div class="contact__widget_address">
                  <h3>Address</h3>
                  
                  <ul>
                      <li><a href="tel:6006LE"><i class="fa fa-phone"></i>6006LE</a></li> 
                      <li><a href="mailto:capela625@gmail.com"><i class="fa fa-envelope"></i>capela625@gmail.com</a></li>
                      <li><p><i class="fa fa-map-marker"></i>Solar System, Earth</p></li>
                  </ul>
              </div>
          </div>
      </div>
      <div class="row contact__footer">
          <div class="col-lg-6">
              <div class="contact__footer_copy">
                  <p>All rights reserved copyright Â© capfab.io 2020</p>
              </div>
          </div>
          <div class="col-lg-6">
              <div class="contact__footer_social">
                  <ul>
                      <li><a href="https://www.linkedin.com/in/fabio-capela-phd-3a19925b/"><i class="fa fa-linkedin-square"></i></a></li>
                      <li><a href="https://github.com/fregocap"><i class="fa fa-github-square"></i></a></li>
                      <li><a href="https://www.instagram.com/capela625/"><i class="fa fa-instagram"></i></a></li>
                  </ul>
              </div>
          </div>
      </div>
  </div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyD0eifSMvIhlNwZ9gEgSGU4kHamEbVcMj4&libraries=geometry"></script>
<script src="http://capfab.io/plugins/jQuery/jquery.min.js"></script>
<script src="http://capfab.io/plugins/bootstrap/bootstrap.min.js"></script>
<script src="http://capfab.io/plugins/slick/slick.min.js"></script>
<script src="http://capfab.io/plugins/slick/slick.min.js"></script>
<script src="http://capfab.io/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="http://capfab.io/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="http://capfab.io/plugins/tweenmax/TweenMax.min.js"></script>
<script src="http://capfab.io/plugins/masonry/masonry.min.js"></script>

<script src="http://capfab.io/js/form-handler.min.js"></script>

<script src="http://capfab.io/js/script.min.js"></script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

  </body>
</html>
