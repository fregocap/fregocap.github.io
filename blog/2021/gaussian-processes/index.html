<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Fabio  Capela | Gaussian Mixture Models</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üî•</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2021/gaussian-processes/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Gaussian Mixture Models",
      "description": "A mixture model is a very powerful (simple) model to represent an overall distribution through $K$ sub-distributions.",
      "published": "December 14, 2021",
      "authors": [
        
        {
          "author": "Fabio Capela",
          "authorURL": "",
          "affiliations": [
            {
              "name": "DLab, Firmenich SA",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Fabio</span>   Capela
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Gaussian Mixture Models</h1>
        <p>A mixture model is a very powerful (simple) model to represent an overall distribution through $K$ sub-distributions.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>In this blog post, we are going to discuss the so-called <em>Gaussian Mixture Models</em>. Such models
are very intuitive and have lots of real applications such as image segmentation, clustering or
even generative modeling.</p>

<h2 id="what-are-gmms-used-for-">What are GMMs used for ?</h2>

<p>The Gaussian distribution is the best known probability distribution in statistics. GMMs leverage
the known properties of the Gaussian distribution to capture/estimate/cluster subparts of the
overall distribution as being distributed locally as a normal/Gaussian distribution. In general,
GMMs do not need to know to what local Gaussian distribution a particular point of the data set
belongs to and are able to estimate it automatically. As such, one can say that GMMs are part
of the large class of unsupervised models.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># first gaussian
</span><span class="n">mean1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">standard_deviation</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># second gaussian
</span><span class="n">mean2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.5</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mean2</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># third gaussian
</span><span class="n">mean3</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mean3</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># overall plotting
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">y2</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">y3</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Coordinates'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p><img src="/assets/img/density_estimation.png" alt="'Density Estimation With Gaussian Process'" /></p>

<p>As one can see from the figure above, we have <strong>three underlying Gaussian distributions</strong> that do
fit well the overall distribution, represented by the plain black line. Estimating the parameters of
the individual Gaussian distributions is at the core of the GMMs modeling.</p>

<p>GMMs have several applications, such as image segmentation, multi-object tracking in video sequences,
audio feature extraction, etc.</p>

<p>One of the things that tell us that the data might be well fitted to be modeled by GMMs is that it
looks multimodal, i.e. we can see from the previous figure several peaks are hinted in the data. An
obvious way to model a multimodal distribution is to consider that parts are generated from unimodal
distributions such as the Gaussian distribution. It turns out that due to mathematical reasons,
the normal/Gaussian distribution is a good unimodal distribution to model a multimodal distribution
in terms of multiple unimodal distributions.</p>

<h2 id="the-model">The Model</h2>

<p>Let us go deeper into the mathematical details needed to represent and code the GMMs. There are
three types of parameters that one needs to be aware of in the GMMs:</p>

<ol>
  <li>
    <p>The mixture/component weights, i.e. the weights that indicate that a particular point (or ith
of a multidimensional point) belongs to a specific component $K$. One can interpret that as the
probability that the point $\mathbf{x}$ belongs to the component $i$, i.e. $\phi_i(\mathbf{x})
\equiv p(c=i|\mathbf{x})$.</p>
  </li>
  <li>
    <p>The means $\mathbf{\mu}_i$ of each Gaussian component. They are represented by a matrix 
$K \times D$ in the case where the point $\mathbf{x}$ is multidimensional of dimension $D$ (vector with $D$
directions) and the dataset distribution is well represented by $K$ Gaussian components.</p>
  </li>
  <li>
    <p>The covariances $\mathbf{\Sigma}_i$ of each Gaussian component. They specify the covariance
structure of each component $i$. Such matrices can be isotropic, i.e. having the same amount of
variance in each direction, diagonal meaning that each variance direction is independent of the
others or a full-rank covariance matrix meaning that a set of directions can be controlled
independently of the others. In practice, we consider a diagonal covariance matrix.</p>
  </li>
</ol>

<p>If we do consider that the component weights are actually the probability that a certain point
$\mathbf{x}$ do belong to a particular component $i$ of the Gaussian mixture model, then we
end with the normalization property that :</p>

\[\sum_{i=1}^K \phi_i = 1\]

<p>In the generic multidimensional case, the Gaussian mixture is defined by the following set of
equations:</p>

\[p(\mathbf{x}) = \sum_{i=1}^K \phi_i(\mathbf{x}) \mathcal{N}(\mathbf{x}|\mathbf{\mu}_i, \mathbf{\Sigma}_i)\]

<p>with</p>

\[\mathcal{N}(\mathbf{x}|\mathbf{\mu}_i, \mathbf{\Sigma}_i) = \frac{1}{\sqrt{(2\pi)^K \left|\Sigma_i\right|}}\exp\left(-\frac{1}{2} (\mathbf{x}-\mathbf{\mu})^\intercal \mathbf{\Sigma_i}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\]

<h2 id="train-the-model">Train the Model</h2>

<p>Now that we have defined the model, we need to understand how to estimate the parameters of the
model. One particular setup that is widely used is the so-called Expectation-Maximization algorithm
(EM). This algorithm is used to find the <strong>maximum likelihood parameters</strong> of a model for the
particular case where an analytical result is unknown. By maximum likelihood, we are considering
that there is a set of parameters that best fit th joint probability of the data sample, i.e. it
is an optimization problem.</p>

<p>The underlying hypothesis of the maximum likelihood algorithm is that all data is observable and
that there is no <em>hidden variables</em> that do control the distribution of the data. Instead, when
there is what we call <strong>latent variables</strong> that do influence certain variables in the dataset,
then a suitable algorithm is actually the EM algorithm. One can say that the EM algorithm is a
maximum likelihood optimization with the presence of latent variables. The introduction of such
latent variables may seem a bit ad-hoc. One can consider them as something needed to simplify the
maximization problem.</p>

<h3 id="expectation-maximization">Expectation Maximization</h3>

<p>One can generically divide the EM algorithm into two steps:</p>

<ol>
  <li>
    <p>Estimation of the latent variables of the model</p>
  </li>
  <li>
    <p>Maximization of the parameters of the model</p>
  </li>
</ol>

<p>In the particular case of GMMs, if one considers the maximum likelihood, we should maximize:</p>

\[\ln p(\mathbf{x}|\phi_i, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^N \ln\left( \sum_{i=1}^K \phi_i(\mathbf{x}^{(n)}) \mathcal{N}(\mathbf{x}^{(n)}|\mathbf{\mu}_i, \mathbf{\Sigma}_i) \right)\]

<p>with respect to $\theta = (\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i)$. However,
two problems arise by doing so: 1) we can have very high (arbitrarily large) likelihood when a
single Gaussian explains a point; 2) an unlimited number of solutions is acceptable up to
permutations. Instead, if we do introduce a latent variable $z$, then one can consider that the
mixture model generates the data by first sampling from $z$, and only then we sample the
observable data $\mathbf{x}$ from a distribution that does depend on $z$, meaning:</p>

\[p(z,\mathbf{x}) = p(z)p(\mathbf{x}|z).\]

<p>In mixture models, the <strong>latent variables are easily interpreted as being the different
components of the data distribution</strong>, i.e. $z=c$ .</p>

<p>Let us try to optimize $\ln p(\mathbf{x})$ for the set of parameters $\theta$ by integrating
over the latent variable</p>

\[\begin{align}
\frac{d}{d\theta} \ln p(\mathbf{x}) &amp; = \frac{d}{d\theta} \ln \sum_{z} p(z, \mathbf{x})  \\
&amp; =  \frac{\frac{d}{d\theta} \sum_{z} p(z,\mathbf{x})}{\sum_{z'}p(z',\mathbf{x})}\\
&amp; = \sum_{z} p(z|\mathbf{x}) \frac{d}{d\theta} \ln p(z,\mathbf{x}) \\
&amp; = \mathbb{E}_{p(z|\mathbf{x})} \left[\frac{d}{d\theta} \ln p(z,\mathbf{x}) \right]. 
\end{align}\]

<p>This means that the derivative of the marginal log-probability $p(\mathbf{x})$ is the expected
value of the derivative of the joint log-probability, with the expectation on the posterior
distribution. This formula is completely generic for any model with latent variables as we
did not introduce any specificities related to GMMs. We have not given the full details of
the derivation, but just keep in mind that we have used the known property:</p>

\[\frac{d}{d\theta} \ln A(\theta) = \frac{1}{A(\theta)} \frac{d}{d\theta} A(\theta).\]

<p>It is rather tempting to equalize the derivative to zero for the particular case of the GMMs.
Doing so, you end up with the optimum parameters that we are looking for. In particular, our
two previous steps of the EM algorithm become:</p>

<ol>
  <li><strong>E-step</strong> corresponds to obtaining the probability of each sample belonging to a particular
component:</li>
</ol>

\[r_{ni} :=p(z_{n} = i | \mathbf{x}_n) = \frac{\phi_i \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_i,\mathbf{\Sigma}_i)}{\sum_{j=1}^K \phi_j \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_j,\mathbf{\Sigma}_j)}\]

<ol>
  <li><strong>M-step</strong> is about obtaining the parameters of the model given the previous step:</li>
</ol>

\[\phi_i = \frac{\sum_{n=1}^N r_{ni}}{\sum_{i=1}^K \sum_{n=1}^N r_{ni}}, \qquad \mathbf{\mu}_i = \frac{\sum_{n=1}^N  r_{ni}\mathbf{x}_n}{\sum_{n=1}^N r_{ni}}, \qquad \Sigma_{i} = \frac{\sum_{n=1}^N r_{ni} \left(\mathbf{x}_n -\mathbf{\mu}_i\right)\left(\mathbf{x}_n - \mathbf{\mu}_i\right)^\intercal}{\sum_{n=1}^N r_{ni}}\]

<p>These two steps define fully the EM algorithm for the GMMs with a random initialization of
the parameters $\theta = \left{\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i\right}$.</p>

<h2 id="clustering-estimation">Clustering Estimation</h2>

<p>Now that we have defined the EM algorithm for the GMMs, we can implement it in the simple case of
a clustering exercise. Basically, we have to make the iterative process explained above until
the difference of the log-likelihood between the new update and the old value is sufficiently close
to consider that the algorithm has converged.</p>

<p>Therefore, to cluster each point of the dataset, we have to:</p>

<ol>
  <li>train the model, i.e. obtain the different parameters of the model, in particular the means
$\mu_i$  and the covariances $\Sigma_i$ ;</li>
  <li>for each point get the value of $r_{ni}$ that, as previously said, corresponds to a data point $x$
belonging to a component $i$.</li>
</ol>

<p>Let us first start by generating two non-trivial distributions:</p>

<pre><code class="language-Python">import matplotlib.pyplot as plt
import numpy as np

mean = [0,0]
cov = [[0.05,0],[0,100]]
cov2 = [[0.5,0],[2,25]]

x,y = np.random.multivariate_normal(mean, cov, 5000).T
x2,y2 = np.random.multivariate_normal(mean, cov2, 5000).T

plt.scatter(x2,y2, alpha=0.2)
plt.scatter(x,y, alpha=0.2)
plt.show()
</code></pre>
<p><img src="/assets/img/gmm_clustering1.png" alt="'Density Estimation With Gaussian Process'" /></p>

<p>As you can see for the second example, we did not consider a diagonal covariance matrix.
That‚Äôs also an interesting case, because there is some overlapping region that will definitely
be challenging for the algorithm to understand.</p>

<p>The stopping criteria is related to the difference between the log-likelihood at a step $ùëõ-1$
and step $ùëõ$  being below a certain threshold. Meanwhile, until that threshold is not reached
we continue updating the estimated parameters of the model. Let‚Äôs therefore built a class with
a <code class="language-plaintext highlighter-rouge">fit</code> method.</p>

<pre><code class="language-Python">from scipy.stats import multivariate_normal
import matplotlib.pyplot as plt

class GMM():
    def __init__(self, n_components, n_iters, threshold = 1e-6, seed = 42):
        
        self.n_components = n_components
        self.threshold = threshold
        self.seed = seed
        self.n_iters = n_iters
        
    def fit(self, X): 
        """ Method that learns the parameters of the GMM
        """
        # initialization of the parameters
        old_log_likelihood = 0
        
        ## rni and weights, i.e. prior
        n_row, n_col = X.shape
        self.rni = np.zeros((n_row, self.n_components))
        self.weights = np.full(self.n_components, 1/self.n_components)
            
        # mean initialization
        np.random.seed(self.seed)
        choice = np.random.choice(n_row,self.n_components, replace=False)
        self.means = X[choice]

        # covariance matrix initialization
        shape_var = self.n_components, n_col, n_col
        self.covariances = np.full(shape_var, np.cov(X, rowvar=False))
        
        # start the main loop
        for i in range(self.n_iters):
            # compute first the log-likelihhod
            self.__log_likelihood(X)
            new_log_likelihood = np.sum(np.log(np.sum(self.rni, axis=1)))
        
            # run the E-M step
            self.__E_step(X)
            self.__M_step(X)
            
            # check convergence 
            if abs(new_log_likelihood - old_log_likelihood) &lt;=self.threshold:
                break
                
            # if it did not converge, then update the log-likelihood
            old_log_likelihood = new_log_likelihood
                        
        
    
    def __E_step(self, X):
        """ Method that implements the E-step
        """
        #normalize over the different cluster probabilities
        self.rni = self.rni/self.rni.sum(axis=1, keepdims=1)
        
    
    def __M_step(self, X):
        """ Method that implements the M-step
        """
        phi_num = self.rni.sum(axis=0)
        phi_i = phi_num /X.shape[0]
        
        # means
        self.means = np.dot(self.rni.T, X)/ phi_num.reshape(-1,1)
        
        #covariances
        for k in range(self.n_components):
            diff = (X - self.means[k]).T
            cov_num = np.dot(self.rni[:,k]*diff, diff.T)
            self.covariances[k] = cov_num / phi_num[k]
        
    
    def __log_likelihood(self,X):
        """ Method to get the log-likelihood
        """
        for k in range(self.n_components):
            prior = self.weights[k]
            likelihood = multivariate_normal(self.means[k], self.covariances[k]).pdf(X)
            self.rni[:,k] = prior * likelihood
            
    
    def plot_component(self, X, title='Clusters'):
        """ Method that plots the different components assigned to the data points X
        """
        plt.figure()
        plt.plot(X[:,0], X[:,1], 'ko', alpha=0.01)
        
        delta = 0.25
        k = self.means.shape[0]
        x = np.arange(-4,4, delta)
        y = np.arange(-40,40,delta)
        x_grid, y_grid = np.meshgrid(x,y)
        coordinates = np.array([x_grid.ravel(), y_grid.ravel()]).T 
        
        col = ['green', 'red']
        for i in range(self.n_components):
            mean = self.means[i]
            cov = self.covariances[i]
            z_grid = multivariate_normal(mean, cov).pdf(coordinates).reshape(x_grid.shape)
            plt.contour(x_grid, y_grid, z_grid, colors = col[i])
            
        plt.title(title)
        plt.tight_layout()

</code></pre>

<p>One can apply such class to our previous dataset and follows the figure below:</p>

<p><img src="/assets/img/gmm_components.png" alt="'Density Estimation With Gaussian Process'" /></p>

<p>We were therefore able to predict the two components, even though there was some clear
overlap between the two distributions. This shows the power of the Gaussian Mixture models.
The big inconvenient of such GMM is that they require the specification of the number of
components/clusters before even launching the training. However, one can use certain information
criterion such as Akaike or the Bayesian. Such criterion will penalize model complexity and
will therefore help you get the minimum number of clusters that fits roughly well the
overall dataset.</p>

<p>Another challenge in the training of such models is that the number of parameters
grows quite rapidly with the size of the dataset, in particular due to the fact that it
needs to estimate the full covariance matrices (one covariance matrix of size $N\times N$ for
each cluster). A simple way to avoid such problem is to consider covariance matrices that are
diagonal (therefore having 0 on the off-diagonal values).</p>

<p>This article is a treatment of the subject mainly for myself, but if you read it until here
I hope you have enjoyed it.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Fabio  Capela.
    
    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
