<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <title>Why Neural Networks Work So Well</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel --> 
  <link rel="stylesheet" href="http://capfab.io/plugins/slick/slick.css" />
  <link rel="stylesheet" href="http://capfab.io/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="http://capfab.io/plugins/font-awesome/css/font-awesome.min.css" />
  <!-- Bootstrap -->
  <link rel="stylesheet" href="http://capfab.io/plugins/bootstrap/bootstrap.min.css" />
  <link rel="stylesheet" href="http://capfab.io/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="http://capfab.io/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="http://capfab.io/images/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="http://capfab.io/images/favicon.png" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-179028456-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-179028456-1');
  </script>
	
</head>

  <body>
    <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
      <a href="http://capfab.io/" class="navbar-brand">
          <img src="http://capfab.io/images/site-navigation/logo.png" alt="site-logo">
      </a>
      <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
          <span class="navbar-toggler-icon"></span>
          <span class="navbar-toggler-icon"></span>
          <span class="navbar-toggler-icon"></span>
      </button>
  
      <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
          <ul class="nav navbar-nav main-navigation">
              
              
                <li class="nav-item">
                  <a href="http://capfab.io/#home" class="nav-link ">Home</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#about" class="nav-link ">About</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#blog" class="nav-link ">Blog</a>
                </li>
              
                <li class="nav-item">
                  <a href="http://capfab.io/#contact" class="nav-link ">Contact</a>
                </li>
              
          </ul>
          <div class="navbar-nav">
              <a href="http://capfab.io/contact" class="hire_button">Let's Talk</a>
          </div>
      </div>
  </div>
</nav>

    <div id="content">
      

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Why Neural Networks Work So Well</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          <ol class="breadcrumb align-items-center">
            <li class="breadcrumb-item"><a href=http://capfab.io/>Home</a></li>
            <li class="breadcrumb-item"><a href=http://capfab.io/blog>All Post</a></li>
            <li class="breadcrumb-item active" aria-current="page">Why Neural Networks Work So Well</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
      <img src=http://capfab.io/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
      
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
          <defs>
              <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
                  <stop offset="0" stop-color="#f1f6f9" />
                  <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
              </linearGradient>
          </defs>
          <g data-name="blob-shape (3)">
              <path class="blob" fill="url(#d)"
                  d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
          </g>
      </svg>
  </div>
  <div class="animate-pattern">
      <img src=http://capfab.io/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
      <div class="row">
          <div class="col-lg-12">
              <div class="singleBlog__feature">
                  <img src=http://capfab.io/images/single-blog/feature-image.jpg alt="feature-image">
              </div>
          </div>
      </div>
      <div class="row mt-5">
          <div class="col-lg-12">
              <div class="singleBlog__content">
                <blockquote>
<p>Machine learning and deep learning (a subset of machine learning ) are everywhere. In the last 10 years,
we got self-driving cars, algorithms that understand language better than humans, machines that beat world
champions on the Go game. The big question still stands: why such systems work so
well ? There is no underlying principle that is guiding data scientists when building such algorithms, apart
from an inspiration coming from our big humain brain. However, as a physicist, I do believe that principles
should be at the core of such constructs, as that would allow to have more reliable and transparent systems.
Those two things, reliability and transparency, must be of the uttermost importance for a society that is
pilling up algorithms that are becoming central in our every day lifes (yes, healthcare is a big topic of
innovation in AI)</p>
</blockquote>
<h1 id="whats-a-neural-network">What's a Neural Network</h1>
<p>What are neural networks anyway? Deep learning and neural networks seem to be a new (and trendy) topic,
but they are actually old, with the first model dating back to 1943 [<a href="#references">McCulloch and Pitts</a>]. It all started with
a simple linear regression:
$$f(x) = \sum_i x_i w_i$$
These rudimentary models would have the weights $w_1, \cdots, w_n$ adjusted by a human for a set of n inputs
$x_1,\cdots,x_n$. Rosenblatt in 1958 [<a href="#references">Rosenblatt</a>] was the first to introduced the concept of learnable weights
through examples coming from the data, the so called perceptron. Therefore, the perceptron is a model that learns
to associate a bunch of inputs $x_i$ to a target $y$. Ideally, we have that $f(x)=y$, even though most of the time
that will not happen, because there is always some inherent noise in the data that the learned function cannot
capture.</p>
<p>Once we understood exactly the perceptron, then neural networks are just a more general framework. Indeed, the perceptron
only captures linear dependencies between the inputs and the output. A simple way to add non-linearities is to apply an activation
function to $f(x)$, e.g.
$$
\sigma(x) = \frac{1}{1+\text{e}^{-x}}.
$$</p>
<p>Such activation function is called <strong>sigmoid</strong>, but it is a choice among many others. ReLU, Tanh and many others are
also widely used. Those functions are called <strong>activation</strong> functions because the perceptron can be perceived as one neuron
that will or not be activated by the inputs. Indeed, with our perceptron we can try to predict a continuous target or
try to classify a binary signal, cancer/no cancer, for example.</p>
<p>You might even be more ambitious and consider the perceptron after applying the activation function as the input of another
perceptron, i.e. mathematically speaking:
$$
g(x) = \sum_j \sigma_j\left(f(x)\right) v_j
$$
where $v_j$ corresponds to the weights of the new perceptron <em>layer</em>. This normally allows for even more representational power,
as you are putting together small bits of abstractions into larger ones. You might end up with something pretty complex depending
on the number of times you combine low-level abstractions. When you actually stack several of those layers together, you are constructing <strong>deep neural networks</strong>.</p>
<p>Data scientists have a way to represent a deep neural network through a diagram that associates the inputs to the output(s). A four
layer neural network might be represented by the following diagram.</p>
<p><img src="../../images/latest-post/neuralnet75.png" alt="Neural Network">
<em>Figure 1. Neural Network with four layers that create different sort of features that will be descriminative to finally be able to classify correctly the animal in an image.
(Image source: <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">QuantaMagazine 2017</a>)</em></p>
<h1 id="the-definitions">The Definitions</h1>
<p>Now that we have put together the essential building blocks of what is a neural network (weights, neurons/perceptrons,
activation functions), we can attempt to answer the initial question. The first thing to notice is that a neural
network, when it classifies an image through the constructions of different features, it is actually building an internal
representation of the distribution of values that differ between one image and the other. Once that work is done, a
classification becomes a trivial exercise. Therefore, the neural networks mainly works with distributions. Before digging
into further details, let's first define certain central elements.</p>
<p>One of the principal tasks that neural networks are doing is to compute the weights $w_i$, previously discussed.
One way that the neural network is doing it is through the maximization of what is called the <strong>likelihood</strong> of the observed data.
To understand the concept of likelihood, let's consider a neural network that needs to classify
images between dogs and cats and the likelihood of the predicted class of a particular observation $j$ is defined as $\hat{y}_j$.
Let's consider the following example</p>
<p>$$
\hat{y} =
\begin{bmatrix}
\text{Cat} &amp; 0.3 \newline
\text{Dog} &amp; 0.7
\end{bmatrix}
$$</p>
<p>In that case, the 0.3 corresponds to the likelihood (or probability) that the image shows a cat and 0.7 is the probability that the image represents a dog.
The ground truth observation is a one-hot vector $y$ where the $j^{\text{th}}$ outcome is 1 and all the other outcomes are 0. In our particular example, we have that</p>
<p>$$
y =
\begin{bmatrix}
\text{Cat} &amp; 0 \newline
\text{Dog} &amp; 1
\end{bmatrix}
$$</p>
<p>From $y$ and $\hat{y}$ we are able to define the likelihood of the observation as being:</p>
<p>$$
\prod_{j=1}^N \hat{y}_j^{y_j} = 0.3^0 \times 0.7^1 = 0.7
$$</p>
<p>As you might have guessed it, the aim of the neural network is to maximize the likelihood of the
observations that we are passing through the neurons (or equivalently
to minimize the negative log-likelihood). This is one of the most important building blocks of
neural networks and it is called the objective function of the neural network. You can define all
sorts of objective functions, but they are in some way connected to the maximization of the likelihood of observations.</p>
<p>Another important concept which is at the foundations of neural networks is <strong>information</strong>. The mathematical theory of information was
mainly developed by Claude Shanon and Warren Weaver. They were two engineers that wanted to model information and understand the role of
noise, entropy and chaos. They defined that for a source $X$ with n symbols $x_i$, where each symbol has probability $p_i$ to be
communicated, the so-called Shannon entropy is defined as</p>
<p>$$
H(X) = -\sum_i p_i \log_2 (p_i).
$$</p>
<p>It intuitively corresponds to the amount of information contained or delivered by a source. That source can correspond to different things,
such as a certain language (e.g. french), an electrical signal or file that is transfered through the internet. Here, I've used the log in
basis 2 as it was initially defined, but log in basis 10 can also be used (and is used extensively throughout the literature).</p>
<p>A concept that is derived from Shannon information is the so-called mutual information that is defined by:</p>
<p>$$
\begin{eqnarray}
I(X;Y) &amp;=&amp; \sum_{x\in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \nonumber \\\
&amp;=&amp; H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X;Y) \nonumber
\end{eqnarray}
$$</p>
<p>for two discrete random variables $(X,Y) \sim p(x,y)$ with marginal distributions $p(x)$ and $p(y)$.
<strong>Mutual information</strong> measures the quantity of statistical dependence between two random variable $X$ and $Y$. Here, $H(Y|X)$ means the information
of the variable $Y$ conditioned on $X$, that is the necessary amount of information to know the behavior of $Y$ when we exactly know the behavior
of $X$. Another variable that is useful and tidly connected to the mutual information is the <strong>Kullback-Leibler (KL) divergence</strong>. Such a KL divergence
plays a central role in different machine learning algorithms and corresponds to a metric that measures how similar two probability distributions are.
Considering two probability distributions $P$ and $Q$, the KL divergence is defined through the integral</p>
<p>$$
D_{\text{KL}}\left(p||q\right) = \sum_{x\in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}.
$$</p>
<p>You can understand how the mutual information is actually connected to the Kullback-Leibler divergence by observing that it typically involves the same
equation structurally. Based on that observation, we can easily conclude that:</p>
<p>$$
I(X;Y) = D_{\text{KL}}\left(p(x,y)||p(x)p(y)\right).
$$</p>
<p>The Kullback-Leibler divergence is also sometimes called the <strong>relative entropy</strong>.</p>
<p>We have defined enough material to pursue further on our initial question: what's so special about neural networks and why do they work so well ?</p>
<h1 id="the-compression-of-information">The Compression of Information</h1>
<h1 id="the-renormalization-group">The Renormalization Group</h1>
<p>There have been several papers that try to link deep learning to coarse graining. Since coarse graining is closely related to the Renormalization Group (RG) flow,
we should definitely follow that route and understand how deep learning systems are connected to it and what is the RG flow for the people that are reading this
blog and don't have a background in physics.</p>
<p>Let's be rather concrete and consider the case of images. In the case of deep learning systems, we want to learn the probability distribution of images, where the
random variable ares the pixel values in a certain neighborhood. In physics, systems are typically studied at equilibrium, where we have a way to encode all the information
through the Hamiltonian. However, the Hamiltonian is mainly useless because it encodes local behavior of the system and can't therefore provide the system's global properties.
Indeed, the Hamiltonian is just made of a sum of local functions. Using short-range interactions with Monte Carlo simulations, one can nonetheless get realistic results, but
at the price of exponential computational costs. That's when the RG flow becomes interesting.</p>
<p>Instead of considering a local Hamiltonian, the RG flow takes the perspective of an effective Hamiltonian, by aggregating neighborhouring sites</p>
<hr>
<p>Cited as:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Bash" data-lang="Bash">
 @article<span style="color:#f92672">{</span>fabio2020neuralnets,
   title   <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Why Neural Networks Work So Well&#34;</span>,
   author  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Capela, Fabio&#34;</span>,
   journal <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;capfab.io/blog&#34;</span>,
   year    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;2020&#34;</span>,
   url     <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://capfab.io/blog/why-neural-networks-work-so-well&#34;</span>
 <span style="color:#f92672">}</span>

</code></pre></div><h2 id="references">References</h2>
<p>[1] McCulloch and Pitts, 1943<br>
[2] Rosenblatt, 1958</p>

		<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "capfabio" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
              </div>
          </div>
      </div>
  </div>
</section>


    </div>
    <section class="contact" id="contact">
  <div class="contact__background_shape">
      <svg viewBox="0 0 1920 79">
          <path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
      </svg>
  </div>
  <div class="container">
      <div class="row">
          <div class="col-lg-12">
              <div class="contact__cta">
                  <div class="shape-1">
                      <svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
                          <path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/>
                      </svg>
                  </div>
                  <div class="shape-2">
                      <svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
                          <path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/>
                      </svg>
                  </div>
                  <div class="contact__cta_content">
                      <span>Contact me</span>
                      <h1>Let's get in touch</h1>
                  </div>
                  <div class="contact__cta_action">
                      <a href="http://capfab.io/contact">Message Me</a>
                  </div>
              </div>
          </div>
      </div>
      <div class="row contact__widget">
          <div class="col-lg-4">
              <div class="contact__widget_logo">
                  <img src="http://capfab.io/images/contact/widget-logo.png" alt="widget-logo">
              </div>
          </div>
          <div class="col-lg-4">
              <div class="contact__widget_sitemap">
                  <h3>Sitemap</h3>
                  <ul>
                      
                      
                        <li><a href="http://capfab.io/about">About me</a></li>
                      
                        <li><a href="http://capfab.io/">Privacy &amp; Policy</a></li>
                      
                        <li><a href="http://capfab.io/">Latest Article</a></li>
                      
                  </ul>
              </div>
          </div>
          <div class="col-lg-4">
              <div class="contact__widget_address">
                  <h3>Address</h3>
                  
                  <ul>
                      <li><a href="tel:6006LE"><i class="fa fa-phone"></i>6006LE</a></li> 
                      <li><a href="mailto:capela625@gmail.com"><i class="fa fa-envelope"></i>capela625@gmail.com</a></li>
                      <li><p><i class="fa fa-map-marker"></i>Solar System, Earth</p></li>
                  </ul>
              </div>
          </div>
      </div>
      <div class="row contact__footer">
          <div class="col-lg-6">
              <div class="contact__footer_copy">
                  <p>All rights reserved copyright © capfab.io 2020</p>
              </div>
          </div>
          <div class="col-lg-6">
              <div class="contact__footer_social">
                  <ul>
                      <li><a href="https://www.linkedin.com/in/fabio-capela-phd-3a19925b/"><i class="fa fa-linkedin-square"></i></a></li>
                      <li><a href="https://github.com/fregocap"><i class="fa fa-github-square"></i></a></li>
                      <li><a href="https://www.instagram.com/capela625/"><i class="fa fa-instagram"></i></a></li>
                  </ul>
              </div>
          </div>
      </div>
  </div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyD0eifSMvIhlNwZ9gEgSGU4kHamEbVcMj4&libraries=geometry"></script>
<script src="http://capfab.io/plugins/jQuery/jquery.min.js"></script>
<script src="http://capfab.io/plugins/bootstrap/bootstrap.min.js"></script>
<script src="http://capfab.io/plugins/slick/slick.min.js"></script>
<script src="http://capfab.io/plugins/slick/slick.min.js"></script>
<script src="http://capfab.io/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="http://capfab.io/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="http://capfab.io/plugins/tweenmax/TweenMax.min.js"></script>
<script src="http://capfab.io/plugins/masonry/masonry.min.js"></script>

<script src="http://capfab.io/js/form-handler.min.js"></script>

<script src="http://capfab.io/js/script.min.js"></script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

  </body>
</html>
