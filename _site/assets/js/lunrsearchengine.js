
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "Quant by Day, Triathlete by Dawn",
    "body": "Hi there! I'm Fabio, a passionate data scientist with a love for turning complex data into actionable insights. Currently, I work on innovative projects as a Senior Strategist at LDC in Geneva, where I design and implement advanced quantitative trading strategies. My journey in data science has been quite an adventure, from developing AI-driven flavors at Firmenich SA to co-founding SamurAI, where we built cutting-edge sentiment analysis tools. Outside of work, I'm an avid triathlete, always chasing the next challenge. Whether it's swimming, biking, or running, I find great joy in pushing my physical limits. When I'm not training, you'll often find me spending quality time with my family, which is incredibly important to me.  My academic background is rooted in theoretical physics, with a PhD from ULB in Brussels. This foundation fuels my enthusiasm for mathematics and physics, constantly driving me to explore and innovate in the field of data science. In addition to my professional and athletic pursuits, I have a deep interest in philosophy. I enjoy reading and contemplating philosophical texts, finding that they provide a rich perspective that complements my scientific endeavors. Buy me a coffee or talk to me (or both)Thank you for your support! Your donation helps me to maintain and improve LabFab . Buy me a coffee Let's Talk"
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                                         Unlocking Creativity: Dalle 2 &amp; Diffusion Models                                                 1 2 3 4 5                                              :               In the rapidly evolving landscape of generative models, diffusion models have emerged as a groundbreaking approach, reshaping our understanding and capabilities in fields like image. . . :                                                                                                                                                                       StackNets                                01 Jan 2024                                                                                                                      All Stories:                                                                                                     Markov Decision Processes                         1 2 3 4 5                      :       In this blog post, we are going to talk about Markov Decision Process (MDP). MDPs are very important in the context of reinforcement learning (RL), because lots of RL problems. . . :                                                                               StackNets                13 Aug 2024                                                                                                                                     Elliptic Curves &amp; Modular Forms                         1 2 3 4 5                      :       Mathematics is often described as the language of the universe, and within this vast language, elliptic curves and modular forms hold a special place. These two seemingly abstract concepts have. . . :                                                                               StackNets                07 Aug 2024                                                                                                                                     Latent Variables &amp; Generative Models                         1 2 3 4 5                      :       In the realm of data science and machine learning, latent variables are like hidden factors or underlying structures in your data that you can’t directly observe. Think of them as. . . :                                                                               StackNets                25 Jun 2024                                                                                                                                     A Simple Proof of the Cauchy-Schwarz Inequality                         1 2 3 4 5                      :       Inner product spaces play a crucial role in various fields such as linear algebra, quantum mechanics and more. One of the key results in these spaces is the Cauchy-Schwarz inequality. . . . :                                                                               StackNets                22 Jun 2024                                                                                                                                     An Intro to Gaussian Mixture Models                         1 2 3 4 5                      :       In this blog post, we will explore the concept of Gaussian Mixture Models (GMMs). These models are intuitive and widely applicable in various domains such as image segmentation, clustering, and. . . :                                                                               StackNets                01 May 2024                                            "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/reinforcement-learning/",
    "title": "Markov Decision Processes",
    "body": "2024/08/13 - In this blog post, we are going to talk about Markov Decision Process (MDP). MDPs are very important in the context of reinforcement learning (RL), because lots of RL problems can be put into some form or another as MDPs. For example, the bandit problem, which is a typical RL problem, is actually an MDP with one state. One can also have MDPs with a full observable environment (e. g. chess) or with partially observable environment (financial market). What are Markov Processes ?: So, what is a Markov Process anyway ? To understand that, we first need toclarify what does Markov means. Markov Property: If you actually take a random variable (S), then the Markov propertytells us that the future values \(S_{t+j}\) of that variable areindependent of the past values \(S_{t-i}\), knowing (or conditioned on)the present value \(S_{t}\). This is a pretty interesting property,because it means that all of the history that happened to that variable isactually irrelevant to the future values of the variable. Imagine a chess game: two of your friends start a game and you let them play, while you cook some delicious meal in the kitchen. Then, one of your friends is kind of tired and does not want to play anymore, but the other is not very happy because he felt he could win the game, and he would like to continue. So, you propose to continue to play, replacing your friend. Should you be aware of all the moves that were done? No. The game is fully characterized by the positions of the pieces at that particular moment. All that really matters for the future of the game is alreadyon the board. Mathematically speaking, if we consider a state \(S_{t}\) (a state,in general, means “information available at a particular instant \(t\)”, in our case the random variable or the chess board with positioning of the pieces), then the probability distribution of the state \(S_{t+1}\) only depends on \(S_{t}\), i. e. [\mathbb{P}\left[S_{t+1} \vert S_{t},S_{t-1},\cdots,S_{1},S_{0} \right] = \mathbb{P}\left[S_{t+1} \vert S_{t}\right]] Another way to understand this is to basically consider that you start at a state \(s\) and you have the next state \(s'\), then the state transition probability is defined by: [\mathbb{P}\left[S_{t+1} = s’ \vert S_t = s \right] = \mathcal{P}_{ss’}. ] Therefore, I can transition to the next state that is completely characterized by the present state. Here, we have that \(S_{t+1}\) is a particular instantiation of \(s'\), while \(S_{t}\) corresponds to the state \(s\). Once we have that state transition probability \(\mathcal{P}_{ss'}\), we can represent a state transition matrix \(\mathcal{P}\), where the index of my rows represent the present state \(s\) where my system is and the index of the columns represents the potential next state where my system might transition to. Therefore, the matrix \(\mathcal{P}\) is represented by: [\begin{equation}\mathcal{P} = \begin{bmatrix}\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \vdots &amp; \ddots &amp; \vdots \mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}\end{bmatrix}\end{equation}] This matrix provides the full information about how the Markov process evolves. We can sample from it and it will provide different kinds of possible evolutions to my system. We are finally equipped to define an MDP: it is a sequence of finite states that are fully characterized by the transition probability matrix \(\mathcal{P}\). Therefore, an MDP can be fully defined by a state space \(\mathcal{S}\) and a transition probability \(\mathcal{P}\). Rewards: Now that we have defined what a Markov process is, we will dig into the decision part of the MDPs. To be able to take good (whatever that might mean) decisions, we will need to introduce a value judgement that is called the reward in the RL framework. Such reward represents what the agent gets when it transitions fromthe state \(s\) to the state \(s'\). We have now a Markov reward process that is defined through the tuple\((\mathcal{S},\mathcal{P},\mathcal{R},\gamma)\), where \(\mathcal{R}\) is defined as a reward function that tells us how much reward we get from the state \(s\), i. e. [\mathcal{R}_s = \mathbb{E}\left[R_t \vert S_t = s\right]] and \(\gamma \in [0,1]\) is a discount factor that considers what’s the importance that we provide to rewards far in the future versus immediate rewards. If we do have \(\gamma = 1\), then we care about all the rewards far into the future, while in the case \(\gamma=0\), we care about the immediate rewards only. Based on that, we can introduce the basic goal in reinforcement learning, which isto maximize the return \(G_t\) that corresponds to the total discounted rewardthat we sum up over all the states through which the system is gonna pass through,i. e. [G_t = R_t + \gamma R_{t+1}+\cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}. ] If we do consider that there is indeed an infinite amount of steps, we do see that the discount factor \(\gamma\) plays as well a very useful mathematical role, which is to make the series finite. We will not dig into it in here though. Value Function: Until now, we did not talk about expectations, because we were considering the case of a particular sample and its corresponding total reward. However, at the end of the day what we do care about is expectations. In the case of MDPs, we talk about value function as providing the long-term value of a state \(s\). Therefore, the expected return from a state \(s\) is [v(s) = \mathbb{E}\left[G_t \vert S_t = s \right]. ] Being in a non-deterministic environment, you don’t know exactly what would be your final total return \(G_t\), but you can compute your expected return based on the transition probability matrix. Bellman Equation: Now that we have defined the value function, we can finally introduce the most important relation in all MDPs: the Bellman equation. The basic idea is to split the reward into two parts: the immediate reward that you get and what comes after that immediate reward. Let’s introduce the definitions of \(G_t\) into the previous defintion \(v(s)\): [\begin{align}v(s) &amp;= \mathbb{E}\left[G_t \vert S_t = s\right] &amp;= \mathbb{E}\left[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2}+\cdots \vert S_t = s\right] &amp;= \mathbb{E}\left[R_t + \gamma \left(R_{t+1} + \gamma R_{t+2}+\cdots\right) \vert S_t = s\right] &amp;= \mathbb{E}\left[R_t + \gamma G_{t+1} \vert S_t = s\right] &amp;= \mathbb{E}\left[R_t + \gamma v\left(S_{t+1}\right) \vert S_t = s \right]\end{align}] You do see a recurrent relation that tells us how good it is to be in a particular state \(s\) depends on the immediate reward plus how good it is to be in the next state with a discounted factor \(\gamma\). Once we have that definition of the value function, we can rewrite it in terms of the transition probability matrix \(\mathcal{P}_{ss'}\) and the reward function \(\mathcal{R}_{s}\) at state \(s\) by just inserting the definitions of the expectations into the equation. That leads to the following relation: [v(s) = \mathcal{R}s + \gamma \sum{s’ \in \mathcal{S}} \mathcal{P}_{ss’}v(s’). ] We can also rewrite that Bellman equation into something that is ratherstraightforward to understand, which are matrices and vectors: [v = \mathcal{R} + \gamma \mathcal{P}v. ] Being a linear equation, we can then invert it, getting: [v = \left(1-\gamma \mathcal{P} \right)^{-1} \mathcal{R}. ] There are a bunch of methods that can be used to solve the Bellman equationfor large MDPs, such as linear programming, Temporal-Difference learning, etc. We might address some of those techniques in future blog posts. Let us now introduce the final essential element of MDPs: actions. The Action Space: A MDP is basically a Markov reward process with decisions, therefore we candefined the MDP through the tuple \(\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R} \rangle\), where \(\mathcal{A}\) is a finite set of actions. Based on the existence of such set of actions, we are now able to actually generalize all of the previous concepts introduced. In particular, we can say that the state transition probability matrix depends not only on the state where you are, but as well on the action that you take: [\mathcal{P}{ss’}^{a} = \mathbb{P}\left[S{t+} = s’ \vert S_t = s, A_t = a\right]. ] Therefore, the probability of ending up on all the possible different states depends on the action that you take at the instant \(t\) and at which state you are at time \(t\). The reward function \(\mathcal{R}\) might as well depend on the action: [\mathcal{R}^{a}{s} = \mathbb{E}\left[R{t+} \vert S_t=s, A_t=a \right]. ] Apart from that, everything is the same. The Policy: We are now well equipped to define what it means to make and take decisions. In order to do that, we need to define what is called a policy. The formal definition of a policy \(\pi\) is a distribution over actions given states: [\pi(a\vert s) = \mathbb{P} \left[A_t = a \vert S_t =s \right]. ] In other words, if our system is in a particular state \(S_t\), what’s theprobability of taking a particular action \(A_t\). Therefore, once you have a policy you have fully defined the behavior of an agent taking action in a particular system (remember the chess player? That’s our agent in that example, i. e. the person taking actions). Another interesting implication of the Markov property is that the policy only depends on the current state (and not the past states) as we discussed for the Chess example. Therefore, the policy is said to be stationary or time-independent. Also the policy depends on the rewards through the state where the system is, because the state where the system is characterized by the immediate and expected future rewards of the agent. What defines a Markov reward process given by a chain of states and rewardsis the averaging over policies of our transition probability matrix and reward function, i. e. [\begin{align}\mathcal{P}^{\pi}_{ss’} &amp;= \sum_{a\in \mathcal{A}} \pi(a \vert s) \mathcal{P}^{a}_{ss’} \mathcal{R}^{\pi}_s &amp;= \sum_{a\in \mathcal{A}} \pi(a \vert s) \mathcal{R}^{a}_{s}\end{align}] As such the Markov reward process corresponds to the tuple \(\langle \mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi} \rangle\). Previously, the value function didn’t have any agent, no way to define actions. We have now a way to choose the value function through the policy \(\pi\) and as such, the state-value function \(v_{\pi}(s)\) of an MDP becomes the expected return from state \(s\) that follows a policy \(\pi\) [v_{\color{red}{\pi}}(s) =\mathbb{E}_{\color{red}{\pi}}\left[G_t \mid S_t = s \right];] we have an expectation \(\mathbb{E}_{\pi}\) over the total return when we sample the actions following the policy \(\pi\). We can also define a second type of function that is the action value function, which tells us how good it is to take a particular action from a particular state. This is the mathematical object that we should consider when we have to take a particular action. Therefore, the action value function is the expected return from a state \(s\), taking an action \(a\) and by following a particular policy \(\pi\): [q_{\pi}(a,s) = \mathbb{E}_{\pi}\left[G_t \vert S_t = s, A_t = a \right]] A new Bellman equation is obtained, as previously, by decomposing the immediate reward plus the value of the next state: [v_{\pi}(s) = \mathbb{E}{\pi}\left[R{t}+\gamma v_{\pi}(S_{t+1}) \vert S_t = s \right]. ] In a similar way, we can get an equation for the action-value function [q_{\pi}(s,a) = \mathbb{E}{\pi} \left[R_t+\gamma q{\pi}(S_{t+1},A_{t+1}) \vert S_t =s \right]. ] That last equation allows to relate the action-value of the next state with respect to the state where my system is right now. The way to underst it a bit better is through the relationship that is present between \(v\) and \(q\). So, in order to get \(v_{\pi}(s)\), we are actually averaging over all the possible actions that we might take in the future. Since each action is really valued by the action-value function \(q_{\pi}\) (at the next state), then we need to average over all the action-values under a certain policy \(\pi\) (since the actions are actually sampled from a particular policy), providing us the value of the present state \(s\), i. e. [v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \vert s) q_{\pi}(s,a). ] Let’s now consider instead that we are going to take a particular action \(a\) at a particular state \(s\). If we take the example of the chess game, we are not asking the question: how good is it to take a specific move, while in the previous paragraph, we were asking how good is it to be where I am now in the game (basically, my probability of winning the game). Therefore, we have to average over the possible states where the action that we are taking are going to lead me (plus the immediate reward), i. e. [q_{\pi}(s,a) = \mathcal{R}^a_{s}+ \gamma \sum_{s’ \in \mathcal{S}}\mathcal{P}^a_{ss’}v_{\pi}(s’). ] If we put the last two equations together, we end up with the following recursive relation [v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \vert s) \left( \mathcal{R}^a_{s}+ \gamma \sum_{s’ \in \mathcal{S}}\mathcal{P}^a_{ss’}v_{\pi}(s’)\right)] which is the new Bellman equation for \(v_{\pi}\) that we were looking for. We can also do the same trick with \(q_{\pi}\) to end up with the following recursive relation: [q_{\pi}(s) = \mathcal{R}^a_{s} + \gamma \sum_{s’ \in \mathcal{S}} \mathcal{P}^a_{ss’} \sum_{a’ \in \mathcal{A}} \pi(a’ \vert s’) q_{\pi}(s’, a’). ] The two equations are actually how we solve the MDPs. If you abstract the math, you understand that the idea behind those two equations are really simple: the value function at the actual step is just the immediate reward plus the value function at the step where you are after taking a particular action. That’s all fine, but what we are looking for the optimal actions to pick. For that, we will need to get the optimal policy. The Optimal Policy: Given a state you are in, you want to pick actions that will provide you the maximum future rewards. That policy is called the optimal policy. Let’s first defined the optimal state-value function \(v_{*}(s)\) as being the maximum value function over all possible policies: [v_{*}(s) = \max_{\pi} v_{\pi}(s),] This function basically tells us what’s the maximum possible reward we can extract from the particular system we are in. In a similar fashion, one can define the optimal action-value function \(q_{*}(s,a)\) as being the maximum action-value function over all policies [q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a),] meaning if you commit to a particular action, what’s the most reward you can get. What’s important to notice is that if we know \(q_{*}\), then we have solved the MDP, because under all policies it allows to understand the maximum reward you can get for a particular action. Therefore, knowing \(q_{*}\) allows us to behave optimally under the MDP. As such, (and again) solving an MDP is actually getting \(q_*\). Bellman Optimality Equation: Of course, that’s all good. But in practice, how do we get \(q_{*}\) anyways ? Well, you need to actually get the Bellman optimality equation and solve it. Before, we were looking at expectation on action and rewards. Now, we are really looking at the maximum returns. So, our two previous equations become: [v_{}(s) = \max_{a}\mathcal{R}^a_{s} + \gamma \sum_{s’ \in \mathcal{S}} \mathcal{P}^{a}_{ss’}v_{}(s’),] [q_{}(s,a) = \mathcal{R}^a_{s} + \gamma \sum_{s’ \in \mathcal{S}} \mathcal{P}^a_{ss’} \max_{a’}q_{}(s’, a’),] based on the fact that an optimal policy is found by maximizing over \(q_{*}(s,a)\), i. e. [\pi_{*}(a\mid s) = \begin{cases} 1 &amp; \text{if } a= \text{arg}\max_{a\in\mathcal{A}}q_{}(s,a), \ 0 &amp; \text{otherwise} . \end{cases}] Now that we have the Bellman optimality equation, we should be done. Unfortunately, things are not that easy because in the last two equations \(q_{*}\) and \(v_{*}\) are not linear anymore :-( . Moreover, there is no closed form solution because there are some max involved that complexifies the problem. So, we have to resort to iterative solution methods such as Q-learning and dynamic programming methods. We will talk about those in a subsequent blog post. Conclusion: So, let’s stop here. We have put in place the foundations for the understanding of a reinforcement learning setup through the study of MDPs. We have defined the Markov property and the closely related process of Markov Reward Processes. Then, after having introduced rewards, we talked about policies and the actions that are sampled from those policies. We derived some (Bellman) equations that allow to connect the (action)-value function at a certain state with the (action)-value functions at a future state. Finally, we approached the subject of optimal policies and how to choose optimal actions by solving the Bellman optimality equation. "
    }, {
    "id": 6,
    "url": "http://localhost:4000/elliptic-curves/",
    "title": "Elliptic Curves & Modular Forms",
    "body": "2024/08/07 - Mathematics is often described as the language of the universe, and within this vast language, elliptic curves and modular forms hold a special place. These two seemingly abstract concepts have deep and surprising connections to number theory, cryptography, and even the proof of some of the most famous conjectures in mathematics. In this blog post, we’ll dive into the intriguing realms of elliptic curves and modular forms, exploring their definitions, properties, and the profound link between them. Part 1 : Elliptic CurvesDefinition and Basic Properties:  Algebraic Definition Geometric Interpretation Examples of Elliptic CurvesElliptic Curve Equation:  Weierstrass Equation Conditions for Non-singularity (Discriminant)Group Law on Elliptic Curves:  Definition of the Group Law Geometric Construction: Chord-Tangent Process Properties of the Group StructureElliptic Curves over Finite Fields:  Definition and Importance Hasse’s Theorem on the Number of Points Applications in Cryptography (ECC)Elliptic Cruve Cryptography (ECC):  Basic Principles Key Exchange Algorithms (e. g. , Diffie-Hellman) Digital Signatures Security Advantages and ApplicationsPart 2: Modular FormsIntroduction to Modular Forms:  Basic Definition and Historical Context Modular Group and Modular TransformationsProperties &amp; Examples:  Holomorphicity Fourier Expansion Eisenstein Series and Delta FunctionModular Forms of Different Weights:  Definitions and Examples Transformational PropertiesApplications in Number Theory:  Connection to L-functions Ramanujan’s Tau Function Partition Functions and Other Arithmetic FunctionsPart 3: The Connection Between Elliptic Curves and Modular FormsIntroduction to the Modularity Theorem:  Historical Background Statement of the TheoremProof of the Modularity Theorem:  Key Ideas and Steps in Wiles’ Proof Importance of Ribet’s TheoremImplications of the Modularity Theorem:  Proof of Fermat’s Last Theorem Broader Impact on Number TheoryElliptic Curves as Modular Forms:  Constructing Modular Forms from Elliptic Curves The Role of Galois RepresentationsPart 4: Advanced Topics and Current ResearchElliptic Curve L-functions:  Definition and Properties Birch and Swinnerton-Dyer ConjectureModular Forms and Galois Representations:  Langlands Program Serre’s ConjectureRecent Developments in Elliptic Curves and Modular Forms:  Cryptographic Innovations Modern Research in Number TheoryConclusion:  Summary of Key Points   The Continuing Journey of Mathematics: Discuss how elliptic curves and modular forms continue to be areads of active research and discovery   References and Further Reading…"
    }, {
    "id": 7,
    "url": "http://localhost:4000/variational-autoencoders/",
    "title": "Latent Variables & Generative Models",
    "body": "2024/06/25 - In the realm of data science and machine learning, latent variables are like hidden factors or underlying structures in your data that you can’t directly observe. Think of them as the unseen forces that shape and influence the observable data. For example, in a dataset of movie ratings, latent variables could represent abstract concepts like genre preferences or viewing habits. Latent Variables and Deep Generative Models: Variational AutoEncoders (VAEs) stand out in their ability to handle complex data like images. At their core, VAEs are based on a directed latent-variable model, expressed as       [p(x,z) = p(x   z)p(z),]   where \(x\) is the observed data (such as an image of a face), and \(z\) represents latent variables, the unseen factors influencing \(x\). In the application of VAEs to facial images, the model adeptly learns a latent space \(z\), which encodes a variety of facial features. This latent space is remarkably versatile. It enables us to interpolate between different facial expressions or other attributes, such as gender, by smoothly varying the values in the latent space. This ability to manipulate latent factors is particularly intriguing because these attributes are not explicitly labeled in the training process. Instead, the model infers them from the data, learning what constitutes a smile, a frown, or any other nuanced facial feature. Although we’re focusing on a single-layer latent model, it’s worth noting that deep generative models can have multiple layers (e. g. , \(p(x\vert z_1)p(z_1 \vert z_2)\cdots p(z_{m-1}\vert p(z_m))\)). These multi-layer models can learn hierarchies of latent representations, capturing more complex and abstract features at each level. However, VAEs face two significant challenges: intractability in computation and handling large datasets. The exact computation of the posterior probability \(p(z\vert x)\) is typically not feasible due to its complexity. To navigate this, VAEs apply a strategy called variational inference. This approach involves using an approximate posterior \(q_{\phi}(z \vert x)\) and tweaking it to minimize its divergence from the true posterior. This approximation is crucial for the model to be both computationally feasible and effective in learning the underlying data distribution. We will come back to that point later. When it comes to handling large datasets, which is a common scenario in image processing, VAEs adapt by using stochastic gradient descent methods that work with small, randomly sampled batches of data. This technique ensures that the model can be trained on large datasets without requiring immense computational resources to process the entire dataset at once. The training process: In the context of VAEs, several traditional methods encounter significant challenges:    EM Algorithm: while the Expectation-Maximization (EM) algorithm is a standard approach for learning latent-variable models, it falters in VAEs because the E-step, which requires computing the approximate posterior \(p(z \vert x)\), is intractable. Additionally, the M-step, involving parameter learning across the entire dataset, is impractical for large datasets, despite some alleviations offered by online EM using mini-batches.     Mean Field Approximation: This technique falls short in VAEs as it requires computing expectations over a Markov blanket, whose size becomes infeasible when every component of x depends on each component of z. The resulting computational complexity scales exponentially, rending this approach impractical.     Sampling-Based Methods: While theoretically sound, sampling-based methods like Metropolis-Hastings struggle to scale to large datasets and require carefully crafted proposal distributions, which are challenging to design.  Auto-Encoding Variational Bayes (AEVB) as a Solution: The Auto-Encoding Variational Bayes (AEVB) algorithm emerges as a robust solution to these challenges. It’s grounded in variational inference principles and efficiently addresses the three key tasks: learning the model parameters, performing approximate posterior inference over \(z\), and handling marginal inference of \(x\).    Evidence Lower Bound (ELBO): The core of AEVB is to maximize the ELBO, \(L(p_{\phi}, q_{\phi}) = \mathbb{E}_{q_{\phi}(z \vert x)}\left[\log p_{\theta}(x,z) - \log q_{\phi}(z \vert x) \right]\). This maximization tightens around the log probability \(\log p_{\theta}(x)\) while optimizing over \(q_{\phi}\).     Optimizing \(q(z \vert x)\): The optimization of \(q(z \vert x)\) in AEVB goes beyond mean field’s limitations. It involves using a broader class of \(q\) distributions, more complex than fully factored ones, and employing gradient descent over \(\phi\), instead of coordinate descent.     Joint Optimization: AEVB simultaneously optimizes both \(\phi\) (to keep the ELBO tight around \(\log p(x)\)) and \(\theta\) (to increase the lower bound and hence \(\log p(x)\)), mirroring the lower-bound optimization in EM but in a more scalable and flexible manner.  The Score Function Gradient Estimator: The gradient computation in AEVB is critical:\(\nabla_{\theta, \phi} \mathbb{E}_{q_{\phi}(z)}\left[\log p_{\theta}(x,z) - \log q_{\phi}(z) \right]\).    Gradient of \(p\): The gradient with respect to \(\theta\) is estimated using Monte Carlo methods by sampling from \(q\). The swap between gradient and expectation is feasible here.     Gradient of \(q\): The challenge is in computing the gradient with respect to \(\phi\), where directly swapping gradient and expectation isn’t possible since the expectation is calculated in relation to the very distribution that is the subject of our differentiation. This is where the reparametrization trick comes into play, enabling efficient and low-variance gradient estimation.  Reformulation of the ELBO: The restructuring of the ELBO can be presented in the following manner:       [\log p(x) \geq \mathbb{E}{q{\phi}(z   x)}\left[ \log p_{\theta}(x   z) - KL(q_{\phi}(z   x)       p(z))\right]]   This formulation can be seen as an alternate yet mathematically equivalent version of the ELBO, deduced through straightforward algebraic steps. Looking at this version, it offers a compelling way to interpret the mechanics of the model. Consider any given observed data point, denoted as \(x\). The formula is composed of two key parts, each involving the generation of a latent variable \(z\) from \(q(z \vert x)\), which can be seen as a unique ‘encoding’ of \(x\). Here, \(q\) acts as the ‘encoder’. The term \(\log p(x \vert z)\) is the log-likelihood of observing \(x\) when given this ‘encoding’ \(z\). The aim is for \(p(x \vert z)\), the ‘decoder network’, to assign a high probability to the true \(x\), efficiently ‘decoding’ or reconstructing \(x\) from \(z\). This process is known as minimizing the ‘reconstruction error’. On the other hand, the Kullback-Leibler (KL) divergence, the second component, measures the deviation of the encoded distribution \(q(z \vert x)\) from a predetermined prior distribution \(p(z)\) , typically a standard Gaussian. This ‘regularization term’ encourages the latent representations \(z\) to adopt a Gaussian distribution. Its purpose is to ensure that \(q(z \vert x)\) goes beyond a simple identity mapping, pushing it to learn richer and more complex representations, such as identifying distinct facial features in image-related tasks. The overarching goal of this optimization is to fine-tune \(q(z \vert x)\) so that it maps \(x\) into a practical and interpretable latent space \(z\), enabling the effective reconstruction of \(x\) using \(p(x \vert z)\) . This objective mirrors the foundational concept of auto-encoder neural networks, which are designed to discover and utilize valuable data representations within a latent space. The reparametrization trick: The reparametrization trick is a crucial component in the VAE framework, primarily because it allows for a more efficient and stable estimation of gradients during the optimization process. This trick was a key innovation in the original VAE paper. To understand how this works, consider the distribution \(q_{\phi}(z \vert x)\), which is the approximate posterior in a VAE. This distribution can be constructed through a two-step process:  Sampling Noise Variable: First,a noise variable \(\epsilon\) is sampled from a simple distribution, typically the standard normal distribution \(\mathcal{N}(0,1)\):[\epsilon \sim p(\epsilon)]  Deterministic Transformation: Next, this noise variable is transformed using a deterministic function \(g_{\phi}(\epsilon,x)\), resulting in the variable \(z\):[z = g_{\phi}(\epsilon,x)] This approach ensures that \(z\), when transformed by \(g_{\phi}\), follows the desired distribution \(q_{\phi}(z \vert x)\). The simplest illustration of the reparametrization trick is with Gaussian variables. Normally, one might express \(z\) as being sampled from a Gaussian distribution \(\mathcal{N}(\mu, \sigma)\): [z\sim q_{\mu,\sigma}(z) = \mathcal{N}(\mu, \sigma)] However, with the reparametrization trick, this is re-expressed as: [z \sim q_{\mu,\sigma}(\epsilon) = \mu + \epsilon \cdot \sigma] where \(\epsilon \sim \mathcal{N}(0,1)\). This reformulation doesn’t change the distribution from which \(z\) is sampled, but it crucially alters how we compute gradients. The major advantage of the reparametrization trick is in computing gradients of expectations with respect to \(q(z)\) for any function \(f\). It allows us to rewrite the gradient as:       [\nabla_{\phi}\mathbb{E}{z\sim q{\phi}(z   x)}\left[f(x,z)\right] = \nabla_{\phi} \mathbb{E}{\epsilon \sim p(\epsilon)} \left[f(x,g{\phi}(\epsilon,x))\right] = \mathbb{E}{\epsilon \sim p(\epsilon)}\left[\nabla{\phi} f(x,g_{\phi}(\epsilon, x))\right]]   This restructed gradient falls inside the expectation, enabling us the use of sampling for estimating the term on the right. This method significantly reduces variance compared to other estimators and is key to effectively learning complex models in VAEs. By placing the gradient inside the expectation, the reparametrization trick not only facilitates a more efficient computation of gradients but also enhances the stability and performance of the learning process in VAEs. Overall: The VAE consists of two primary components: an encoder that maps inputs to a latent space and a decoder that reconstructs inputs from this latent space. Let’s summarize how each component is structured and operates within the VAE framework. Encoder Architecture:    Purpose and Function: The encoder in a VAE is responsible for transforming input data into a representation in the latent space. For an input \(x\), the encoder outputs parameters of the probability distribution of the latent variables \(z\), typically the mean and variance.     Network Design: The encoder is usually a neural network. In the context of image processing, this might be a Convolutional Neural Network (CNN) that can effectively capture spatial hierarchies in pixels. For sequential data like text, Recurrent Neural Networks (RNNs) or Transformers might be used.     Output: The key output of the encoder is two sets of values corresponding to each dimension of the latent space: the means (\(\mu\)) and variances (\(\sigma^2\)) or standard deviations (\(\sigma\)). These define a Gaussian distribution for each latent variable, from which we sample to obtain the latent representation.     Reparametrization Trick: To allow for backpropagation through the stochastic sampling process, the reparametrization trick is used. Instead of sampling \(z\) directly from the distribution defined by \(\mu\) and \(\sigma\), \(z\) is expressed as \(\mu + \epsilon \cdot \sigma\), where \(\epsilon\) is sampled from a standard normal distribution.  Decoder Architecture:    Purpose and Function: The decoder serves the opposite role of the encoder. It takes the latent representation \(z\) and reconstructs the input data \(x\). The aim is to generate data that is as close as possible to the original input.     Network Design: The decoder is also a neural network, often mirroring the architecture of the encoder but in reverse. For images, this might involve deconvolutional layers (also known as transposed convolutions) to upsample from the latent representation to the original data dimensions. For sequential data, the decoder could be an RNN or a Transformer generating one element of the sequence at a time.     Output: The decoder outputs a reconstruction of the original input data. In the case of images, this output is typically the same size as the input image, with each output unit representing a pixel’s properties (like color intensity).     Objective Function: The performance of the decoder is evaluated based on how well the reconstructed data matches the original input. This is often measured by a reconstruction loss, such as mean squared error for continuous data or cross-entropy loss for binary or categorical data.  In summary, the encoder learns to compress data into a compact latent representation, capturing the essential features, while the decoder learns to reconstruct the original data from this compressed form. This architecture allows VAEs to not only generate new data similar to the inputs but also to learn deep representations of the data, useful for various applications like anomaly detection, data denoising, and more. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/cauchy-schwarz/",
    "title": "A Simple Proof of the Cauchy-Schwarz Inequality",
    "body": "2024/06/22 - Inner product spaces play a crucial role in various fields such as linear algebra, quantum mechanics and more. One of the key results in these spaces is the Cauchy-Schwarz inequality. This blog post explores this inequality and demonstrates its proof using the so-called amplification method. Inner Product Spaces: An inner product space is a vector space equipped with an additional structure called an inner product. This inner product allows for the measurement of angles and lengths within the space. For complex vectors \(\mathbf{u}=(u_1,u_2,\cdots, u_n)\) and \(\mathbf{v}=(v_1,v_2,\cdots,v_n)\), the inner product is defined as: [\langle \mathbf{u}, \mathbf{v} \rangle = \sum_{i=1} \bar{u_i} v_i] where \(\bar{u_i}\) denotes the complex conjugate of \(u_i\). The Cauchy-Schwarz Inequality: The Cauchy-Schwarz inequality states that for any vectors \(\mathbf{u}\) and \(\mathbf{v}\) in an inner product space: [| \langle\mathbf{u}, \mathbf{v} \rangle| \leq \lVert \mathbf{u} \rVert \cdot \lVert \mathbf{v} \rVert] where \(\lVert\mathbf{u}\rVert = \sqrt{\langle \mathbf{u}, \mathbf{u} \rangle}\) and \(\lVert\mathbf{v}\rVert = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}\). This inequality essentially states that the absolute value of the inner product of two vectors is always less than or equal to the product of their norms. The Proof: To prove the Cauchy-Schwarz inequality, we follow these steps: let’s define a new vector \(\mathbf{w} = \mathbf{u} - \alpha \mathbf{v}\), where \(\alpha\) is a real number and let’s compute the inner product of \(\mathbf{w}\) with itself (i. e. the non-negative squared norm): [\begin{align}\langle\mathbf{w}, \mathbf{w} \rangle &amp; = \langle \mathbf{u} - \alpha \mathbf{v}, \mathbf{u} - \alpha \mathbf{v} \rangle &amp; =\langle \mathbf{u}, \mathbf{u} \rangle - \alpha \langle \mathbf{u}, \mathbf{v} \rangle - \alpha \langle \mathbf{v}, \mathbf{u} \rangle + \alpha ^2 \langle \mathbf{v}, \mathbf{v} \rangle \ &amp; = \lVert \mathbf{u} \rVert ^2 -\alpha \langle \mathbf{u}, \mathbf{v} \rangle -\alpha \overline{\langle \mathbf{u}, \mathbf{v} \rangle}+ \alpha ^2 \lVert \mathbf{v} \rVert ^2&amp; = \lVert \mathbf{u} \rVert ^2 -2 \alpha \mathbf{Re}\left(\langle \mathbf{u}, \mathbf{v} \rangle \right)+ \alpha ^2\lVert \mathbf{v} \rVert ^2 \geq 0 . \end{align}] We have an inequality and approaching the expression we want. One interesting thing: norm of vectors are preserved by complex rotations \(v\rightarrow e^{i\theta} v\), but the real part is not. That is, [\mathbf{Re}\left(e^{i\theta}\langle \mathbf{u}, \mathbf{v} \rangle\right) \leq \frac{\alpha}{2} \lVert e^{i\theta} \mathbf{v} \rVert ^2 + \frac{1}{2\alpha} \lVert \mathbf{u} \rVert ^2. ] By choosing the \(\theta\) that turns the left hand side to a real number (i. e. maximizes it), the previous equation becomes:       [\left   \langle \mathbf{u}, \mathbf{v} \rangle\right   \leq \frac{\alpha}{2} \lVert \mathbf{v} \rVert ^2 + \frac{1}{2\alpha} \lVert \mathbf{u} \rVert ^2. ]   The final trick is to fix \(\alpha\) to be given by \(\alpha=\lVert \mathbf{u} \rVert/\lVert \mathbf{v} \rVert\), which minimizes the expression on the right hand side (to convince yourself, just take the derivative wrt to \(\alpha\) and find the \(\alpha\) that minimizes it) for any non-zero \(\mathbf{u}\), \(\mathbf{v}\). That finishes the proof. Conclusion: The amplification method, as demonstrated in this proof of the Cauchy-Schwarz inequality, beautifully showcases the geometric and algebraic connections inherent in this technique. Originally highlighted in ablog post by Terence Tao, this approach provides a clear and elegant pathway to understanding the depth and utility of the inequality. While seemingly straightforward, the method underscores a powerful concept in mathematical analysis, proving to be both insightful and practical for various applications. "
    }, {
    "id": 9,
    "url": "http://localhost:4000/gaussian-processes/",
    "title": "An Intro to Gaussian Mixture Models",
    "body": "2024/05/01 - In this blog post, we will explore the concept of Gaussian Mixture Models (GMMs). These models are intuitive and widely applicable in various domains such as image segmentation, clustering, and generative modeling. Introduction to GMMs: Gaussian Mixture Models are used to model an overall distribution through multiple Gaussian distributions. They are a powerful tool for capturing, estimating, and clustering parts of an overall distribution as locally Gaussian-distributed. GMMs are unsupervised models, meaning they do not need to know the specific Gaussian distribution a data point belongs to in advance. Example of GMM: Here’s a simple example to illustrate GMMs: 1234567891011121314151617181920212223242526import matplotlib. pyplot as pltimport numpy as npfrom scipy import stats# first gaussianmean1 = 0standard_deviation = 1x = np. arange(-10,10,0. 1)y1 = stats. norm(mean1, standard_deviation)# second gaussianmean2 = -2. 5y2 = stats. norm(mean2, standard_deviation)# third gaussianmean3 = 2. 5y3 = stats. norm(mean3, standard_deviation)# overall plottingplt. plot(x, y1. pdf(x), '--', c='gray')plt. plot(x, y2. pdf(x), '--', c='gray')plt. plot(x, y3. pdf(x), '--', c='gray')plt. plot(x, y1. pdf(x)+y2. pdf(x)+y3. pdf(x), c='black')plt. xlabel('Coordinates')plt. ylabel('Density')plt. show() This example shows three Gaussian distributions fitting an overall distribution, with the black line representing the combined distribution. Application of GMMs: GMMs have numerous applications, including:  Image segmentation Multi-object tracking in videos Audio feature extractionThey are particularly useful for multimodal distributions, where multiple peaks are present. These peaks can be modeled using multiple Gaussian distributions. Mathematical Formulation of GMMs: To represent GMMs mathematically, we need to understand three types of parameters:  Mixture Weights (\(\phi\)): indicate the probability that a point belongs to a specific Gaussian component \(K\).  Means (\(\mu\)): the centers of each Gaussian component.  Covariances (\(\mathbf{\Sigma}_i\)): describe the spread and orientation of each Gaussian component. The probability density function for a GMM is given by:       [p(\mathbf{x}) = \sum_{i=1}^K \phi_i(\mathbf{x}) \mathcal{N}(\mathbf{x}   \mathbf{\mu}_i, \mathbf{\Sigma}_i)]   where \(\mathcal{N}(\mathbf{x}\vert\mathbf{\mu}_i, \mathbf{\Sigma}_i)\) is the multivariate Gaussian distribution. Training the GMM: Expectation-Maximization algorithm: The EM algorithm is used to find the maximum likelihood parameters of a GMM, especially when there are latent variables influencing the data distribution. Steps of the EM Algorithm:  Expectation (E-step): Estimate the latent variables.  Maximization (M-step): Maximize the parameters based on the current estimates of the latent variables. In the particular case of GMMs, if one considers the maximum likelihood, we should maximize:       [\ln p(\mathbf{x}   \phi_i, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^N \ln\left( \sum_{i=1}^K \phi_i(\mathbf{x}^{(n)}) \mathcal{N}(\mathbf{x}^{(n)}   \mathbf{\mu}_i, \mathbf{\Sigma}_i) \right)]   with respect to \(\theta = (\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i)\). However,two problems arise by doing so: 1) we can have very high (arbitrarily large) likelihood when asingle Gaussian explains a point; 2) an unlimited number of solutions is acceptable up topermutations. Instead, if we do introduce a latent variable \(z\), then one can consider that themixture model generates the data by first sampling from \(z\), and only then we sample theobservable data \(\mathbf{x}\) from a distribution that does depend on \(z\), meaning:       [p(z,\mathbf{x}) = p(z)p(\mathbf{x}   z). ]   In mixture models, the latent variables are easily interpreted as being the differentcomponents of the data distribution, i. e. \(z=c\) . Let us try to optimize \(\ln p(\mathbf{x})\) for the set of parameters \(\theta\) by integratingover the latent variable [\begin{align}\frac{d}{d\theta} \ln p(\mathbf{x}) &amp; = \frac{d}{d\theta} \ln \sum_{z} p(z, \mathbf{x}) &amp; = \frac{\frac{d}{d\theta} \sum_{z} p(z,\mathbf{x})}{\sum_{z’}p(z’,\mathbf{x})}&amp; = \sum_{z} p(z|\mathbf{x}) \frac{d}{d\theta} \ln p(z,\mathbf{x}) &amp; = \mathbb{E}_{p(z|\mathbf{x})} \left[\frac{d}{d\theta} \ln p(z,\mathbf{x}) \right]. \end{align}] This means that the derivative of the marginal log-probability \(p(\mathbf{x})\) is the expectedvalue of the derivative of the joint log-probability, with the expectation on the posteriordistribution. This formula is completely generic for any model with latent variables as wedid not introduce any specificities related to GMMs. We have not given the full details ofthe derivation, but just keep in mind that we have used the known property: [\frac{d}{d\theta} \ln A(\theta) = \frac{1}{A(\theta)} \frac{d}{d\theta} A(\theta). ] It is rather tempting to equalize the derivative to zero for the particular case of the GMMs. Doing so, you end up with the optimum parameters that we are looking for. In particular, ourtwo previous steps of the EM algorithm become: E-Step:       [r_{ni} :=p(z_{n} = i   \mathbf{x}_n) = \frac{\phi_i \mathcal{N}(\mathbf{x}_n   \mathbf{\mu}i,\mathbf{\Sigma}_i)}{\sum{j=1}^K \phi_j \mathcal{N}(\mathbf{x}_n   \mathbf{\mu}_j,\mathbf{\Sigma}_j)}]   M-Step: [\begin{align}\phi_i &amp; = \frac{\sum_{n=1}^N r_{ni}}{\sum_{i=1}^K \sum_{n=1}^N r_{ni}}, \mathbf{\mu}i &amp; = \frac{\sum{n=1}^N r_{ni}\mathbf{x}n}{\sum{n=1}^N r_{ni}}, \Sigma_{i} &amp; = \frac{\sum_{n=1}^N r_{ni} \left(\mathbf{x}n -\mathbf{\mu}_i\right)\left(\mathbf{x}_n - \mathbf{\mu}_i\right)^\intercal}{\sum{n=1}^N r_{ni}}\end{align}] These two steps define fully the EM algorithm for the GMMs with a random initialization ofthe parameters \(\theta = \left\{\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i\right\}\). Clustering with GMMs: To use GMMs for clustering, follow these steps:  Train the model to obtain the parameters (means, covariances).  Assign each data point to a Gaussian component based on the probability \(r_{ni}\). Let us first start by generating two non-trivial distributions: 12345678910111213import matplotlib. pyplot as pltimport numpy as npmean = [0,0]cov = [[0. 05,0],[0,100]]cov2 = [[0. 5,0],[2,25]]x,y = np. random. multivariate_normal(mean, cov, 5000). Tx2,y2 = np. random. multivariate_normal(mean, cov2, 5000). Tplt. scatter(x2,y2, alpha=0. 2)plt. scatter(x,y, alpha=0. 2)plt. show() As you can see for the second example, we did not consider a diagonal covariance matrix. That’s also an interesting case, because there is some overlapping region that will definitelybe challenging for the algorithm to understand. The stopping criteria is related to the difference between the log-likelihood at a step \(𝑛-1\)and step \(𝑛\) being below a certain threshold. Meanwhile, until that threshold is not reachedwe continue updating the estimated parameters of the model. Let’s therefore built a class witha fit method. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class GMM():  def __init__(self, n_components, n_iters, threshold = 1e-6, seed = 42):        self. n_components = n_components    self. threshold = threshold    self. seed = seed    self. n_iters = n_iters      def fit(self, X):         Method that learns the parameters of the GMM           # initialization of the parameters    old_log_likelihood = 0        ## rni and weights, i. e. prior    n_row, n_col = X. shape    self. rni = np. zeros((n_row, self. n_components))    self. weights = np. full(self. n_components, 1/self. n_components)          # mean initialization    np. random. seed(self. seed)    choice = np. random. choice(n_row,self. n_components, replace=False)    self. means = X[choice]    # covariance matrix initialization    shape_var = self. n_components, n_col, n_col    self. covariances = np. full(shape_var, np. cov(X, rowvar=False))        # start the main loop    for i in range(self. n_iters):      # compute first the log-likelihhod      self. __log_likelihood(X)      new_log_likelihood = np. sum(np. log(np. sum(self. rni, axis=1)))          # run the E-M step      self. __E_step(X)      self. __M_step(X)            # check convergence       if abs(new_log_likelihood - old_log_likelihood) &lt;=self. threshold:        break              # if it did not converge, then update the log-likelihood      old_log_likelihood = new_log_likelihood                    def __E_step(self, X):        Method that implements the E-step           #normalize over the different cluster probabilities    self. rni = self. rni/self. rni. sum(axis=1, keepdims=1)        def __M_step(self, X):        Method that implements the M-step           phi_num = self. rni. sum(axis=0)    phi_i = phi_num /X. shape[0]        # means    self. means = np. dot(self. rni. T, X)/ phi_num. reshape(-1,1)        #covariances    for k in range(self. n_components):      diff = (X - self. means[k]). T      cov_num = np. dot(self. rni[:,k]*diff, diff. T)      self. covariances[k] = cov_num / phi_num[k]        def __log_likelihood(self,X):        Method to get the log-likelihood           for k in range(self. n_components):      prior = self. weights[k]      likelihood = multivariate_normal(self. means[k], self. covariances[k]). pdf(X)      self. rni[:,k] = prior * likelihood          def plot_component(self, X, title='Clusters'):        Method that plots the different components assigned to the data points X           plt. figure()    plt. plot(X[:,0], X[:,1], 'ko', alpha=0. 01)        delta = 0. 25    k = self. means. shape[0]    x = np. arange(-4,4, delta)    y = np. arange(-40,40,delta)    x_grid, y_grid = np. meshgrid(x,y)    coordinates = np. array([x_grid. ravel(), y_grid. ravel()]). T         col = ['green', 'red']    for i in range(self. n_components):      mean = self. means[i]      cov = self. covariances[i]      z_grid = multivariate_normal(mean, cov). pdf(coordinates). reshape(x_grid. shape)      plt. contour(x_grid, y_grid, z_grid, colors = col[i])          plt. title(title)    plt. tight_layout()One can apply such class to our previous dataset and follows the figure below: Challenges and Considerations: GMMs require specifying the number of components beforehand. Model selection criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can help determine the optimal number of components. Additionally, the complexity of GMMs increases with the size of the dataset, particularly due to the covariance matrices. Simplifying assumptions, such as diagonal covariance matrices, can mitigate this issue. In summary, Gaussian Mixture Models are a robust tool for modeling and clustering complex data distributions. Their ability to handle multimodal distributions makes them valuable in various practical applications. "
    }, {
    "id": 10,
    "url": "http://localhost:4000/diffusion-models/",
    "title": "Unlocking Creativity: Dalle 2 & Diffusion Models",
    "body": "2024/01/01 - In the rapidly evolving landscape of generative models, diffusion models have emerged as a groundbreaking approach, reshaping our understanding and capabilities in fields like image synthesis and beyond. Let’s delve into this fascinating world, breaking down complex concepts into digestible insights. Understanding the basics: At their core, diffusion models are a type of generative model, a family of algorithms designed to produce new data samples that mimic a given distribution. Unlike their predecessors (like GANs or VAEs), diffusion models operate on a novel principle: they systematically add noise to data and then learn to reverse this process. The Process: From Noise to Clarity: Diffusion models are inspired by the natural diffusion process, where particles move from regions of higher concentration to lower concentration until they are evenly distributed. In the context of generative modeling, this concept is metaphorically applied to data. The model starts with a piece of structured data, like an image or a text, and gradually introduces random noise over several iterations or steps. This process known as forward diffusion, incrementally adds Gaussian noise until the original data transforms into a state of pure, unstructured noise. One of the most striking aspects of diffusion models is their gradual, stepwise approach to both deterioration and reconstruction. Unlike some generative models that attempt to generate data in a single step, diffusion models embrace a more nuanced path. This gradualism allows for a more controlled generation process and often results in higher-quality, more diverse samples. It’s a journey through a landscape of noise and structure, where each step is a careful move towards creating something new and unique from the vestiges of the old. 1. Forward Diffusion - The Art of Structured Deterioration: The forward diffusion process is methodical and controlled, typically occurring over hundreds or even thousands of steps. At each step, a small amount of Gaussian noise is added according to the equation \(\mathbf{x}_t = \sqrt(1-\beta_t)\mathbf{x}_{t-1}+\sqrt{\beta_t}\epsilon\) where \(\mathbf{x}_t\) represents data at timestamp \(t\), \(\beta_t\) is the noise variance, and \(\epsilon\) is a standard normal sample. This gradual addition of noise, governed by a predefined variance schedule, carefully obscures the data’s structure while retaining latent traits of the initial input, ultimately transforming it into a purely noisy state. As previously said, the amount of Gaussian noise added at each step is governed by a fixed variance schedule with the following predefined properties: [\begin{equation}  q(\mathbf{x}t | \mathbf{x}{t-1}) = \mathcal{N}\left(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_t, \beta_t \mathbf{I} \right)\end{equation}] As \(t\) nears \(T\), the sample \(\mathbf{x}_0\) progressively loses its distinct characteristics, eventually becoming fully noised, such that \(\mathbf{x}_T\) aligns with a Gaussian distribution. Essentially, \(q(\mathbf{x}_t \vert \mathbf{x}_{t-1})\) denotes the transition probability distribution, or the noise added, between \(\mathbf{x}_{t-1}\) and \(\mathbf{x}_t\).  2. Reverse Diffusion - Reclaiming Order from Chaos: Reverse Diffusion is where diffusion models unveil their true capability. After forward diffusion has transformed the data into noise, reverse diffusion meticulously retraces this path, reconstructing the original data from its noisy state. This process, governed by a learned reverse Markov chain, involves a series of probabilistic steps that gradually subtract the noise, guided by the equation: [\begin{equation}p_{\theta}(\mathbf{x}{t-1}\vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}{t-1}; \mu_{\theta}(\mathbf{x}t, t),\Sigma{\theta}(\mathbf{x}_t, t))\end{equation}] where \(\mu_{\theta}\) is the predicted mean, and \(\Sigma_{\theta}\) is the predicted variance of the noise that was added to the data. The variance is usually fixed in practice. Therefore, to estimate the target denoising step, it is sufficient to approximate the mean of the noise added during the forward process. This is achieved through a denoising neural network, which specifically focuses on predicting this noise mean at each step.  This iterative denoising is not a simple inversion but a complex, learned reconstruction. The model, trained through advanced deep learning techniques, has learned the intricate dance of adding and removing noise. Each step in the reverse process is a delicate balance, seeking to predict and correct the previous step’s output, effectively moving from randomness back to the structured data. This journey from noise back to structured data is a testament to the model’s power, demonstrating its ability to not just mimic but creatively re-interpret the original data distribution. It’s this capability that allows diffusion models to generate new, coherent samples that are not mere replicas but variations of the original data, reflecting both the model’s understanding and its creative potential. As the reverse diffusion unfolds, the data passes through various stages of clarity, offering a fascinating glimpse into the model’s learning process and the subtle ways it captures and redefines the data’s inherent patterns. This process doesn’t just generate images or text; it reshapes our understanding of what’s possible in the realm of generative AI, opening new doors for exploration and discovery. 3. Training: Training diffusion models involves a detailed mathematical process, centered on optimization which involves adjusting the model parameters to minimize the difference between the original data and its reconstruction from noise. The training objective can be expressed as minimizing the sum of Kullback-Leibler (KL) divergences between the forward and reverse processes at each timestep. [\begin{equation}L_{t-1} = D_{\text{KL}}(q(\mathbf{x}{t-1}\vert \mathbf{x}{t}, \mathbf{x}{0}) \vert \vert p{\theta}(\mathbf{x}{t-1} \vert \mathbf{x}{t}))\end{equation}] Essentially, this means (as previously said) the model learns to approximate the noise added during the forward process. While the derivation of the training is intricate, it essentially boils down to minimizing the discrepancy between the actual denoising at each step \(t\) and the noise predicted by a neural network \(\epsilon_{\theta}(\mathbf{x}_{t},t)\). This network, adjustable during training, takes the current noised sample \(\mathbf{x}_{t}\) and the time step \(t\) as inputs, aiming to closely replicate the true denoising process. The loss, after a lengthy calculation, takes a simpler form: [\begin{equation}L_{\text{simple}} = \mathbb{E}{\mathbf{x}{0}, \epsilon_t} \left[\Vert \epsilon_t - \epsilon_{\theta}(\mathbf{x}_{t}, t) \Vert^2\right]\end{equation}] By training the model to minimize this divergence, it learns to reverse the diffusion process accurately, enabling the generation of coherent and high-quality data samples from noise. The Architecture: U-Net: The U-Net architecture, pivotal in the training of diffusion models, is renowned for its effectiveness in processing and reconstructing complex data structures. Originating from biomedical image segmentation, U-Net features a distinctive symmetric encoder-decoder structure enhanced by skip connections, which facilitate the flow of information and gradients through the network. This design enables U-Net to capture both high-level overview and fine-grained details of the input data, making it particulary adept at understanding and reconstructing the intricate patterns necessary for reverse diffusion processes.  In the context of diffusion models, U-Net is commonly employed to predict the noise at each timestep or directly estimate the denoised data. Its capability to handle multi-scale features and maintain spatial hierarchies is crucial in enabling diffusion models to gradually refine their outputs, step by step, leading to the generation of coherent and high-fidelity samples. The use of U-Net in diffusion models represent a harmonious blend of architecture and algorithm, where the structure of U-Net complements the iterative nature of the diffusion process, ensuring that each step towards denoising is informed and precise. As research progresses, the adaptability and effectiveness of U-Net in various modifications and improvements signify its continuing importance in the evolving landscape of generative models. Practical Applications: A Universe of Possibilities: Diffusion models aren’t just theoretical marvels; they have practical implications across various domains:    Image Generation: OpenAI’s DALL-E 2 can generate diverse and intricate images from textual descriptions, and Google’s Imagen is known for producing photorealistic images. These models demonstrate the power of diffusion techniques in creating high-quality visual content from a variety of inputs.     Temporal Data Modeling: Diffusion models have been proved to be useful for forecasting and understanding sequences in areas like climate science, where they help model weather patterns over time. Their ability to understand and generate complex sequences makes them valuable for analyzing trends and making predictions in various time-sensitive domains.    Interdiscplinary Applications: Diffusion models offer an innovative approach to molecular design, iteratively refining noise into structured molecules. Unlike traditional models that assume atom independence or require arbitrary atom ordering, diffusion models enable a gradual, controlled design process, similar to AlphaFold2’s method of refining protein structures. This capability allows for the correction of errors and leads to more accurate molecular configurations, revolutionizing the field of computational chemistry. Read more on the subject in Hoogeboom et al. 2022 and in Schneuing et al. 2023 . The Future: Boundless Horizons: The future of diffusion models is as exciting as their present. Ongoing research focuses on enhancing their efficiency, accuracy, and adaptability to different data structures. As these models continue to evolve, we can expect them to unlock even more applications, potentially transforming entire industries. Diffusion models stand as a testament to the incredible progress in machine learning and AI. They blend complex mathematical theory with practical utility, pushing the boundaries of what’s possible in data generation and beyond. As we continue to explore and refine these models, we stand on the brink of a new era in generative modeling, full of possibilities yet to be discovered. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});