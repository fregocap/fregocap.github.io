
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "Quant by Day, Triathlete by Dawn",
    "body": "Hi there! I'm Fabio, a passionate data scientist with a love for turning complex data into actionable insights. Currently, I work on innovative projects as a Senior Strategist at LDC in Geneva, where I design and implement advanced quantitative trading strategies. My journey in data science has been quite an adventure, from developing AI-driven flavors at Firmenich SA to co-founding SamurAI, where we built cutting-edge sentiment analysis tools. Outside of work, I'm an avid triathlete, always chasing the next challenge. Whether it's swimming, biking, or running, I find great joy in pushing my physical limits. When I'm not training, you'll often find me spending quality time with my family, which is incredibly important to me.  My academic background is rooted in theoretical physics, with a PhD from ULB in Brussels. This foundation fuels my enthusiasm for mathematics and physics, constantly driving me to explore and innovate in the field of data science. In addition to my professional and athletic pursuits, I have a deep interest in philosophy. I enjoy reading and contemplating philosophical texts, finding that they provide a rich perspective that complements my scientific endeavors. Buy me a coffee or talk to me (or both)Thank you for your support! Your donation helps me to maintain and improve LabFab . Buy me a coffee Let's Talk"
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                 Unlocking Creativity: Dalle 2 &amp; Diffusion Models                                                 1 2 3 4 5                                              :               In the rapidly evolving landscape of generative models, diffusion models have emerged as a groundbreaking approach, reshaping our understanding and capabilities in fields like image. . . :                                                                                                                                                                       StackNets                                01 Jan 2024                                                                                                                      All Stories:                                                                                                     An Intro to Gaussian Mixture Models                         1 2 3 4 5                      :       In this blog post, we will explore the concept of Gaussian Mixture Models (GMMs). These models are intuitive and widely applicable in various domains such as image segmentation, clustering, and. . . :                                                                               StackNets                01 May 2024                                            "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/gaussian-processes/",
    "title": "An Intro to Gaussian Mixture Models",
    "body": "2024/05/01 - In this blog post, we will explore the concept of Gaussian Mixture Models (GMMs). These models are intuitive and widely applicable in various domains such as image segmentation, clustering, and generative modeling. Introduction to GMMs: Gaussian Mixture Models are used to model an overall distribution through multiple Gaussian distributions. They are a powerful tool for capturing, estimating, and clustering parts of an overall distribution as locally Gaussian-distributed. GMMs are unsupervised models, meaning they do not need to know the specific Gaussian distribution a data point belongs to in advance. Example of GMM: Here‚Äôs a simple example to illustrate GMMs: 1234567891011121314151617181920212223242526import matplotlib. pyplot as pltimport numpy as npfrom scipy import stats# first gaussianmean1 = 0standard_deviation = 1x = np. arange(-10,10,0. 1)y1 = stats. norm(mean1, standard_deviation)# second gaussianmean2 = -2. 5y2 = stats. norm(mean2, standard_deviation)# third gaussianmean3 = 2. 5y3 = stats. norm(mean3, standard_deviation)# overall plottingplt. plot(x, y1. pdf(x), '--', c='gray')plt. plot(x, y2. pdf(x), '--', c='gray')plt. plot(x, y3. pdf(x), '--', c='gray')plt. plot(x, y1. pdf(x)+y2. pdf(x)+y3. pdf(x), c='black')plt. xlabel('Coordinates')plt. ylabel('Density')plt. show() This example shows three Gaussian distributions fitting an overall distribution, with the black line representing the combined distribution. Application of GMMs: GMMs have numerous applications, including:  Image segmentation Multi-object tracking in videos Audio feature extractionThey are particularly useful for multimodal distributions, where multiple peaks are present. These peaks can be modeled using multiple Gaussian distributions. Mathematical Formulation of GMMs: To represent GMMs mathematically, we need to understand three types of parameters:  Mixture Weights (\(\phi\)): indicate the probability that a point belongs to a specific Gaussian component \(K\).  Means (\(\mu\)): the centers of each Gaussian component.  Covariances (\(\mathbf{\Sigma}_i\)): describe the spread and orientation of each Gaussian component. The probability density function for a GMM is given by:       [p(\mathbf{x}) = \sum_{i=1}^K \phi_i(\mathbf{x}) \mathcal{N}(\mathbf{x}   \mathbf{\mu}_i, \mathbf{\Sigma}_i)]   where \(\mathcal{N}(\mathbf{x}\vert\mathbf{\mu}_i, \mathbf{\Sigma}_i)\) is the multivariate Gaussian distribution. Training the GMM: Expectation-Maximization algorithm: The EM algorithm is used to find the maximum likelihood parameters of a GMM, especially when there are latent variables influencing the data distribution. Steps of the EM Algorithm:  Expectation (E-step): Estimate the latent variables.  Maximization (M-step): Maximize the parameters based on the current estimates of the latent variables. In the particular case of GMMs, if one considers the maximum likelihood, we should maximize:       [\ln p(\mathbf{x}   \phi_i, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^N \ln\left( \sum_{i=1}^K \phi_i(\mathbf{x}^{(n)}) \mathcal{N}(\mathbf{x}^{(n)}   \mathbf{\mu}_i, \mathbf{\Sigma}_i) \right)]   with respect to \(\theta = (\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i)\). However,two problems arise by doing so: 1) we can have very high (arbitrarily large) likelihood when asingle Gaussian explains a point; 2) an unlimited number of solutions is acceptable up topermutations. Instead, if we do introduce a latent variable \(z\), then one can consider that themixture model generates the data by first sampling from \(z\), and only then we sample theobservable data \(\mathbf{x}\) from a distribution that does depend on \(z\), meaning:       [p(z,\mathbf{x}) = p(z)p(\mathbf{x}   z). ]   In mixture models, the latent variables are easily interpreted as being the differentcomponents of the data distribution, i. e. \(z=c\) . Let us try to optimize \(\ln p(\mathbf{x})\) for the set of parameters \(\theta\) by integratingover the latent variable [\begin{align}\frac{d}{d\theta} \ln p(\mathbf{x}) &amp; = \frac{d}{d\theta} \ln \sum_{z} p(z, \mathbf{x}) &amp; = \frac{\frac{d}{d\theta} \sum_{z} p(z,\mathbf{x})}{\sum_{z‚Äô}p(z‚Äô,\mathbf{x})}&amp; = \sum_{z} p(z|\mathbf{x}) \frac{d}{d\theta} \ln p(z,\mathbf{x}) &amp; = \mathbb{E}_{p(z|\mathbf{x})} \left[\frac{d}{d\theta} \ln p(z,\mathbf{x}) \right]. \end{align}] This means that the derivative of the marginal log-probability \(p(\mathbf{x})\) is the expectedvalue of the derivative of the joint log-probability, with the expectation on the posteriordistribution. This formula is completely generic for any model with latent variables as wedid not introduce any specificities related to GMMs. We have not given the full details ofthe derivation, but just keep in mind that we have used the known property: [\frac{d}{d\theta} \ln A(\theta) = \frac{1}{A(\theta)} \frac{d}{d\theta} A(\theta). ] It is rather tempting to equalize the derivative to zero for the particular case of the GMMs. Doing so, you end up with the optimum parameters that we are looking for. In particular, ourtwo previous steps of the EM algorithm become: E-Step:       [r_{ni} :=p(z_{n} = i   \mathbf{x}_n) = \frac{\phi_i \mathcal{N}(\mathbf{x}_n   \mathbf{\mu}i,\mathbf{\Sigma}_i)}{\sum{j=1}^K \phi_j \mathcal{N}(\mathbf{x}_n   \mathbf{\mu}_j,\mathbf{\Sigma}_j)}]   M-Step: [\begin{align}\phi_i &amp; = \frac{\sum_{n=1}^N r_{ni}}{\sum_{i=1}^K \sum_{n=1}^N r_{ni}}, \mathbf{\mu}i &amp; = \frac{\sum{n=1}^N r_{ni}\mathbf{x}n}{\sum{n=1}^N r_{ni}}, \Sigma_{i} &amp; = \frac{\sum_{n=1}^N r_{ni} \left(\mathbf{x}n -\mathbf{\mu}_i\right)\left(\mathbf{x}_n - \mathbf{\mu}_i\right)^\intercal}{\sum{n=1}^N r_{ni}}\end{align}] These two steps define fully the EM algorithm for the GMMs with a random initialization ofthe parameters \(\theta = \left\{\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i\right\}\). Clustering with GMMs: To use GMMs for clustering, follow these steps:  Train the model to obtain the parameters (means, covariances).  Assign each data point to a Gaussian component based on the probability \(r_{ni}\). Let us first start by generating two non-trivial distributions: 12345678910111213import matplotlib. pyplot as pltimport numpy as npmean = [0,0]cov = [[0. 05,0],[0,100]]cov2 = [[0. 5,0],[2,25]]x,y = np. random. multivariate_normal(mean, cov, 5000). Tx2,y2 = np. random. multivariate_normal(mean, cov2, 5000). Tplt. scatter(x2,y2, alpha=0. 2)plt. scatter(x,y, alpha=0. 2)plt. show() As you can see for the second example, we did not consider a diagonal covariance matrix. That‚Äôs also an interesting case, because there is some overlapping region that will definitelybe challenging for the algorithm to understand. The stopping criteria is related to the difference between the log-likelihood at a step \(ùëõ-1\)and step \(ùëõ\) being below a certain threshold. Meanwhile, until that threshold is not reachedwe continue updating the estimated parameters of the model. Let‚Äôs therefore built a class witha fit method. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class GMM():  def __init__(self, n_components, n_iters, threshold = 1e-6, seed = 42):        self. n_components = n_components    self. threshold = threshold    self. seed = seed    self. n_iters = n_iters      def fit(self, X):         Method that learns the parameters of the GMM           # initialization of the parameters    old_log_likelihood = 0        ## rni and weights, i. e. prior    n_row, n_col = X. shape    self. rni = np. zeros((n_row, self. n_components))    self. weights = np. full(self. n_components, 1/self. n_components)          # mean initialization    np. random. seed(self. seed)    choice = np. random. choice(n_row,self. n_components, replace=False)    self. means = X[choice]    # covariance matrix initialization    shape_var = self. n_components, n_col, n_col    self. covariances = np. full(shape_var, np. cov(X, rowvar=False))        # start the main loop    for i in range(self. n_iters):      # compute first the log-likelihhod      self. __log_likelihood(X)      new_log_likelihood = np. sum(np. log(np. sum(self. rni, axis=1)))          # run the E-M step      self. __E_step(X)      self. __M_step(X)            # check convergence       if abs(new_log_likelihood - old_log_likelihood) &lt;=self. threshold:        break              # if it did not converge, then update the log-likelihood      old_log_likelihood = new_log_likelihood                    def __E_step(self, X):        Method that implements the E-step           #normalize over the different cluster probabilities    self. rni = self. rni/self. rni. sum(axis=1, keepdims=1)        def __M_step(self, X):        Method that implements the M-step           phi_num = self. rni. sum(axis=0)    phi_i = phi_num /X. shape[0]        # means    self. means = np. dot(self. rni. T, X)/ phi_num. reshape(-1,1)        #covariances    for k in range(self. n_components):      diff = (X - self. means[k]). T      cov_num = np. dot(self. rni[:,k]*diff, diff. T)      self. covariances[k] = cov_num / phi_num[k]        def __log_likelihood(self,X):        Method to get the log-likelihood           for k in range(self. n_components):      prior = self. weights[k]      likelihood = multivariate_normal(self. means[k], self. covariances[k]). pdf(X)      self. rni[:,k] = prior * likelihood          def plot_component(self, X, title='Clusters'):        Method that plots the different components assigned to the data points X           plt. figure()    plt. plot(X[:,0], X[:,1], 'ko', alpha=0. 01)        delta = 0. 25    k = self. means. shape[0]    x = np. arange(-4,4, delta)    y = np. arange(-40,40,delta)    x_grid, y_grid = np. meshgrid(x,y)    coordinates = np. array([x_grid. ravel(), y_grid. ravel()]). T         col = ['green', 'red']    for i in range(self. n_components):      mean = self. means[i]      cov = self. covariances[i]      z_grid = multivariate_normal(mean, cov). pdf(coordinates). reshape(x_grid. shape)      plt. contour(x_grid, y_grid, z_grid, colors = col[i])          plt. title(title)    plt. tight_layout()One can apply such class to our previous dataset and follows the figure below: Challenges and Considerations: GMMs require specifying the number of components beforehand. Model selection criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can help determine the optimal number of components. Additionally, the complexity of GMMs increases with the size of the dataset, particularly due to the covariance matrices. Simplifying assumptions, such as diagonal covariance matrices, can mitigate this issue. In summary, Gaussian Mixture Models are a robust tool for modeling and clustering complex data distributions. Their ability to handle multimodal distributions makes them valuable in various practical applications. "
    }, {
    "id": 6,
    "url": "http://localhost:4000/diffusion-models/",
    "title": "Unlocking Creativity: Dalle 2 & Diffusion Models",
    "body": "2024/01/01 - In the rapidly evolving landscape of generative models, diffusion models have emerged as a groundbreaking approach, reshaping our understanding and capabilities in fields like image synthesis and beyond. Let‚Äôs delve into this fascinating world, breaking down complex concepts into digestible insights. Understanding the basics: At their core, diffusion models are a type of generative model, a family of algorithms designed to produce new data samples that mimic a given distribution. Unlike their predecessors (like GANs or VAEs), diffusion models operate on a novel principle: they systematically add noise to data and then learn to reverse this process. The Process: From Noise to Clarity: Diffusion models are inspired by the natural diffusion process, where particles move from regions of higher concentration to lower concentration until they are evenly distributed. In the context of generative modeling, this concept is metaphorically applied to data. The model starts with a piece of structured data, like an image or a text, and gradually introduces random noise over several iterations or steps. This process known as forward diffusion, incrementally adds Gaussian noise until the original data transforms into a state of pure, unstructured noise. One of the most striking aspects of diffusion models is their gradual, stepwise approach to both deterioration and reconstruction. Unlike some generative models that attempt to generate data in a single step, diffusion models embrace a more nuanced path. This gradualism allows for a more controlled generation process and often results in higher-quality, more diverse samples. It‚Äôs a journey through a landscape of noise and structure, where each step is a careful move towards creating something new and unique from the vestiges of the old. 1. Forward Diffusion - The Art of Structured Deterioration: The forward diffusion process is methodical and controlled, typically occurring over hundreds or even thousands of steps. At each step, a small amount of Gaussian noise is added according to the equation \(\mathbf{x}_t = \sqrt(1-\beta_t)\mathbf{x}_{t-1}+\sqrt{\beta_t}\epsilon\) where \(\mathbf{x}_t\) represents data at timestamp \(t\), \(\beta_t\) is the noise variance, and \(\epsilon\) is a standard normal sample. This gradual addition of noise, governed by a predefined variance schedule, carefully obscures the data‚Äôs structure while retaining latent traits of the initial input, ultimately transforming it into a purely noisy state. As previously said, the amount of Gaussian noise added at each step is governed by a fixed variance schedule with the following predefined properties: [\begin{equation}  q(\mathbf{x}t | \mathbf{x}{t-1}) = \mathcal{N}\left(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_t, \beta_t \mathbf{I} \right)\end{equation}] As \(t\) nears \(T\), the sample \(\mathbf{x}_0\) progressively loses its distinct characteristics, eventually becoming fully noised, such that \(\mathbf{x}_T\) aligns with a Gaussian distribution. Essentially, \(q(\mathbf{x}_t \vert \mathbf{x}_{t-1})\) denotes the transition probability distribution, or the noise added, between \(\mathbf{x}_{t-1}\) and \(\mathbf{x}_t\).  2. Reverse Diffusion - Reclaiming Order from Chaos: Reverse Diffusion is where diffusion models unveil their true capability. After forward diffusion has transformed the data into noise, reverse diffusion meticulously retraces this path, reconstructing the original data from its noisy state. This process, governed by a learned reverse Markov chain, involves a series of probabilistic steps that gradually subtract the noise, guided by the equation: [\begin{equation}p_{\theta}(\mathbf{x}{t-1}\vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}{t-1}; \mu_{\theta}(\mathbf{x}t, t),\Sigma{\theta}(\mathbf{x}_t, t))\end{equation}] where \(\mu_{\theta}\) is the predicted mean, and \(\Sigma_{\theta}\) is the predicted variance of the noise that was added to the data. The variance is usually fixed in practice. Therefore, to estimate the target denoising step, it is sufficient to approximate the mean of the noise added during the forward process. This is achieved through a denoising neural network, which specifically focuses on predicting this noise mean at each step.  This iterative denoising is not a simple inversion but a complex, learned reconstruction. The model, trained through advanced deep learning techniques, has learned the intricate dance of adding and removing noise. Each step in the reverse process is a delicate balance, seeking to predict and correct the previous step‚Äôs output, effectively moving from randomness back to the structured data. This journey from noise back to structured data is a testament to the model‚Äôs power, demonstrating its ability to not just mimic but creatively re-interpret the original data distribution. It‚Äôs this capability that allows diffusion models to generate new, coherent samples that are not mere replicas but variations of the original data, reflecting both the model‚Äôs understanding and its creative potential. As the reverse diffusion unfolds, the data passes through various stages of clarity, offering a fascinating glimpse into the model‚Äôs learning process and the subtle ways it captures and redefines the data‚Äôs inherent patterns. This process doesn‚Äôt just generate images or text; it reshapes our understanding of what‚Äôs possible in the realm of generative AI, opening new doors for exploration and discovery. 3. Training: Training diffusion models involves a detailed mathematical process, centered on optimization which involves adjusting the model parameters to minimize the difference between the original data and its reconstruction from noise. The training objective can be expressed as minimizing the sum of Kullback-Leibler (KL) divergences between the forward and reverse processes at each timestep. [\begin{equation}L_{t-1} = D_{\text{KL}}(q(\mathbf{x}{t-1}\vert \mathbf{x}{t}, \mathbf{x}{0}) \vert \vert p{\theta}(\mathbf{x}{t-1} \vert \mathbf{x}{t}))\end{equation}] Essentially, this means (as previously said) the model learns to approximate the noise added during the forward process. While the derivation of the training is intricate, it essentially boils down to minimizing the discrepancy between the actual denoising at each step \(t\) and the noise predicted by a neural network \(\epsilon_{\theta}(\mathbf{x}_{t},t)\). This network, adjustable during training, takes the current noised sample \(\mathbf{x}_{t}\) and the time step \(t\) as inputs, aiming to closely replicate the true denoising process. The loss, after a lengthy calculation, takes a simpler form: [\begin{equation}L_{\text{simple}} = \mathbb{E}{\mathbf{x}{0}, \epsilon_t} \left[\Vert \epsilon_t - \epsilon_{\theta}(\mathbf{x}_{t}, t) \Vert^2\right]\end{equation}] By training the model to minimize this divergence, it learns to reverse the diffusion process accurately, enabling the generation of coherent and high-quality data samples from noise. The Architecture: U-Net: The U-Net architecture, pivotal in the training of diffusion models, is renowned for its effectiveness in processing and reconstructing complex data structures. Originating from biomedical image segmentation, U-Net features a distinctive symmetric encoder-decoder structure enhanced by skip connections, which facilitate the flow of information and gradients through the network. This design enables U-Net to capture both high-level overview and fine-grained details of the input data, making it particulary adept at understanding and reconstructing the intricate patterns necessary for reverse diffusion processes.  In the context of diffusion models, U-Net is commonly employed to predict the noise at each timestep or directly estimate the denoised data. Its capability to handle multi-scale features and maintain spatial hierarchies is crucial in enabling diffusion models to gradually refine their outputs, step by step, leading to the generation of coherent and high-fidelity samples. The use of U-Net in diffusion models represent a harmonious blend of architecture and algorithm, where the structure of U-Net complements the iterative nature of the diffusion process, ensuring that each step towards denoising is informed and precise. As research progresses, the adaptability and effectiveness of U-Net in various modifications and improvements signify its continuing importance in the evolving landscape of generative models. Practical Applications: A Universe of Possibilities: Diffusion models aren‚Äôt just theoretical marvels; they have practical implications across various domains:    Image Generation: OpenAI‚Äôs DALL-E 2 can generate diverse and intricate images from textual descriptions, and Google‚Äôs Imagen is known for producing photorealistic images. These models demonstrate the power of diffusion techniques in creating high-quality visual content from a variety of inputs.     Temporal Data Modeling: Diffusion models have been proved to be useful for forecasting and understanding sequences in areas like climate science, where they help model weather patterns over time. Their ability to understand and generate complex sequences makes them valuable for analyzing trends and making predictions in various time-sensitive domains.    Interdiscplinary Applications: Diffusion models offer an innovative approach to molecular design, iteratively refining noise into structured molecules. Unlike traditional models that assume atom independence or require arbitrary atom ordering, diffusion models enable a gradual, controlled design process, similar to AlphaFold2‚Äôs method of refining protein structures. This capability allows for the correction of errors and leads to more accurate molecular configurations, revolutionizing the field of computational chemistry. Read more on the subject in Hoogeboom et al. 2022 and in Schneuing et al. 2023 . The Future: Boundless Horizons: The future of diffusion models is as exciting as their present. Ongoing research focuses on enhancing their efficiency, accuracy, and adaptability to different data structures. As these models continue to evolve, we can expect them to unlock even more applications, potentially transforming entire industries. Diffusion models stand as a testament to the incredible progress in machine learning and AI. They blend complex mathematical theory with practical utility, pushing the boundaries of what‚Äôs possible in data generation and beyond. As we continue to explore and refine these models, we stand on the brink of a new era in generative modeling, full of possibilities yet to be discovered. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});