<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">

<title>Markov Decision Processes | LabFab</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Markov Decision Processes | LabFab</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Markov Decision Processes" />
<meta name="author" content="stacknets" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Markov Decision Process is a mathematical framework used in reinforcement learning to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. An MDP consists of states, transition probabilities and rewards. The goal is to find a policy that maximizes the cumulative reward over time." />
<meta property="og:description" content="A Markov Decision Process is a mathematical framework used in reinforcement learning to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. An MDP consists of states, transition probabilities and rewards. The goal is to find a policy that maximizes the cumulative reward over time." />
<link rel="canonical" href="http://localhost:4000/reinforcement-learning/" />
<meta property="og:url" content="http://localhost:4000/reinforcement-learning/" />
<meta property="og:site_name" content="LabFab" />
<meta property="og:image" content="http://localhost:4000/assets/images/reinforcement_learning.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-13T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/images/reinforcement_learning.jpg" />
<meta property="twitter:title" content="Markov Decision Processes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"stacknets"},"dateModified":"2024-08-13T00:00:00+02:00","datePublished":"2024-08-13T00:00:00+02:00","description":"A Markov Decision Process is a mathematical framework used in reinforcement learning to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. An MDP consists of states, transition probabilities and rewards. The goal is to find a policy that maximizes the cumulative reward over time.","headline":"Markov Decision Processes","image":"http://localhost:4000/assets/images/reinforcement_learning.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/reinforcement-learning/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"stacknets"},"url":"http://localhost:4000/reinforcement-learning/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="LabFab">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="/about">About</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li> -->


                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">LabFab</h1>
    <p class="lead">
        Exploring math, physics, machine learning, and finance insights.
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Markov Decision Processes&url=http://localhost:4000/reinforcement-learning/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/reinforcement-learning/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/reinforcement-learning/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="/assets/images/avatar.png" alt="StackNets">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://labfab.io">StackNets</a><a target="_blank" href="https://twitter.com/capelfabio" class="btn follow">Follow</a>
                        <span class="author-description">I'm interested in machine learning, trading and running. I'm currently learning how to make the best focaccia possible and looking to collaborate on any project that makes me grow (though perhaps not in that order).</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Markov Decision Processes</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid" src="/assets/images/reinforcement_learning.jpg" alt="Markov Decision Processes">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#what-are-markov-processes-">What are Markov Processes ?</a>
    <ul>
      <li><a href="#markov-property">Markov Property</a></li>
    </ul>
  </li>
  <li><a href="#rewards">Rewards</a></li>
  <li><a href="#value-function">Value Function</a></li>
  <li><a href="#bellman-equation">Bellman Equation</a></li>
  <li><a href="#the-action-space">The Action Space</a></li>
  <li><a href="#the-policy">The Policy</a></li>
  <li><a href="#the-optimal-policy">The Optimal Policy</a></li>
  <li><a href="#bellman-optimality-equation">Bellman Optimality Equation</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p>In this blog post, we are going to talk about Markov Decision Process (MDP). MDPs are very important in the context of reinforcement learning (RL), because lots of RL problems can be put into some form or another as MDPs. For example, the bandit problem, which is a typical RL problem, is actually an MDP with one state. One can also have MDPs with a full observable environment (e.g. chess) or with partially observable environment (financial market).</p>

<h2 id="what-are-markov-processes-">What are Markov Processes ?</h2>

<p>So, what is a Markov Process anyway ? To understand that, we first need to
clarify what does Markov means.</p>

<h3 id="markov-property">Markov Property</h3>

<p>If you actually take a random variable (S), then the Markov property
tells us that the future values \(S_{t+j}\) of that variable are
independent of the past values \(S_{t-i}\), knowing (or conditioned on)
the present value \(S_{t}\). This is a pretty interesting property,
because it means that all of the history that happened to that variable is
actually irrelevant to the future values of the variable. Imagine a chess game: two of your friends start a game and you let them play, while you cook some delicious meal in the kitchen. Then, one of your friends is kind of tired and does not want to play anymore, but the other is not very happy because he felt he could win the game, and he would like to continue. So, you propose to continue to play, replacing your friend. Should you be aware of all the moves that were done? No. The game is fully characterized by the positions of the pieces at that particular moment. All that really matters for the future of the game is already
on the board.</p>

<p>Mathematically speaking, if we consider a state \(S_{t}\) (a state,
in general, means “information available at a particular instant \(t\)”, in our case the random variable or the chess board with positioning of the pieces), then the probability distribution of the state \(S_{t+1}\) only depends on \(S_{t}\), i.e.</p>

\[\mathbb{P}\left[S_{t+1} \vert S_{t},S_{t-1},\cdots,S_{1},S_{0} \right] = \mathbb{P}\left[S_{t+1} \vert S_{t}\right]\]

<p>Another way to understand this is to basically consider that you start at a state \(s\) and you have the next state \(s'\), then the state transition probability is defined by:</p>

\[\mathbb{P}\left[S_{t+1} = s' \vert S_t = s \right] = \mathcal{P}_{ss'}.\]

<p>Therefore, I can transition to the next state that is completely characterized by the present state. Here, we have that \(S_{t+1}\) is a particular instantiation of \(s'\), while \(S_{t}\) corresponds to the state \(s\).</p>

<p>Once we have that state transition probability \(\mathcal{P}_{ss'}\), we can represent a state transition matrix \(\mathcal{P}\), where the index of my rows represent the present state \(s\) where my system is and the index of the columns represents the potential next state where my system might transition to. Therefore, the matrix \(\mathcal{P}\) is represented by:</p>

\[\begin{equation*}
\mathcal{P} = 
\begin{bmatrix}
\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}
\end{bmatrix}
\end{equation*}\]

<p>This matrix provides the full information about how the Markov process evolves. We can sample from it and it will provide different kinds of possible evolutions to my system.</p>

<p>We are finally equipped to define an MDP: it is a sequence of finite states that are fully characterized by the transition probability matrix \(\mathcal{P}\). Therefore, an MDP can be fully defined by a state space \(\mathcal{S}\) and a transition probability \(\mathcal{P}\).</p>

<h2 id="rewards">Rewards</h2>

<p>Now that we have defined what a Markov process is, we will dig into the decision part of the MDPs. To be able to take good (whatever that might mean) decisions, we will need to introduce a value judgement that is called the <strong>reward</strong> in the RL framework. Such reward represents what the <em>agent</em> gets when it transitions from
the state \(s\) to the state \(s'\).</p>

<p>We have now a <em>Markov reward process</em>  that is defined through the tuple
\((\mathcal{S},\mathcal{P},\mathcal{R},\gamma)\), where \(\mathcal{R}\) is defined as a reward function that tells us how much reward we get from the state \(s\), i.e.</p>

\[\mathcal{R}_s = \mathbb{E}\left[R_t \vert S_t = s\right]\]

<p>and \(\gamma \in [0,1]\) is a discount factor that considers what’s the importance that we provide to rewards far in the future versus immediate rewards. If we do have \(\gamma = 1\), then we care about all the rewards far into the future, while in the case \(\gamma=0\), we care about the immediate rewards only.</p>

<p>Based on that, we can introduce the basic goal in reinforcement learning, which is
to maximize the return \(G_t\) that corresponds to the total discounted reward
that we sum up over all the states through which the system is gonna pass through,
i.e.</p>

\[G_t = R_t + \gamma R_{t+1}+\cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}.\]

<p>If we do consider that there is indeed an infinite amount of steps, we do see that the discount factor \(\gamma\) plays as well a very useful mathematical role, which is to make the series finite. We will not dig into it in here though.</p>

<h2 id="value-function">Value Function</h2>

<p>Until now, we did not talk about expectations, because we were considering the case of a particular sample and its corresponding total reward. However, at the end of the day what we do care about is expectations. In the case of MDPs, we talk about <strong>value function</strong> as providing the long-term value of a state \(s\). Therefore, the expected return from a state \(s\) is</p>

\[v(s) = \mathbb{E}\left[G_t \vert  S_t = s \right].\]

<p>Being in a non-deterministic environment, you don’t know exactly what would be your final total return \(G_t\), but you can compute your expected return based on the transition probability matrix.</p>

<h2 id="bellman-equation">Bellman Equation</h2>

<p>Now that we have defined the value function, we can finally introduce the most important relation in all MDPs: the Bellman equation. The basic idea is to split the reward into two parts: the immediate reward that you get and what comes after that immediate reward. Let’s introduce the definitions of \(G_t\) into the previous defintion \(v(s)\):</p>

\[\begin{align*}
v(s) &amp;= \mathbb{E}\left[G_t \vert S_t = s\right] \\
&amp;= \mathbb{E}\left[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2}+\cdots \vert S_t = s\right] \\
&amp;= \mathbb{E}\left[R_t + \gamma \left(R_{t+1} + \gamma R_{t+2}+\cdots\right) \vert S_t = s\right] \\
&amp;= \mathbb{E}\left[R_t + \gamma G_{t+1} \vert S_t = s\right] \\
&amp;= \mathbb{E}\left[R_t + \gamma v\left(S_{t+1}\right) \vert S_t = s \right]\\
\end{align*}\]

<p>You do see a recurrent relation that tells us how good it is to be in a particular state \(s\) depends on the immediate reward plus how good it is to be in the next state with a discounted factor \(\gamma\).</p>

<p>Once we have that definition of the value function, we can rewrite it in terms of the transition probability matrix \(\mathcal{P}_{ss'}\) and the reward function \(\mathcal{R}_{s}\) at state \(s\) by just inserting the definitions of the expectations into the equation. That leads to the following relation:</p>

\[v(s) = \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}v(s').\]

<p>We can also rewrite that Bellman equation into something that is rather
straightforward to understand, which are matrices and vectors:</p>

\[v = \mathcal{R} + \gamma \mathcal{P}v.\]

<p>Being a linear equation, we can then invert it, getting:</p>

\[v = \left(1-\gamma \mathcal{P} \right)^{-1} \mathcal{R}.\]

<p>There are a bunch of methods that can be used to solve the Bellman equation
for large MDPs, such as linear programming, Temporal-Difference learning, etc. We might address some of those techniques in future blog posts.</p>

<p>Let us now introduce the final essential element of MDPs: actions.</p>

<h2 id="the-action-space">The Action Space</h2>

<p>A MDP is basically a Markov reward process with decisions, therefore we can
defined the MDP through the tuple \(\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R} \rangle\), where \(\mathcal{A}\) is a finite set of actions. Based on the existence of such set of actions, we are now able to actually generalize all of the previous concepts introduced. In particular, we can say that the state transition probability matrix depends not only on the state where you are, but as well on the action that you take:</p>

\[\mathcal{P}_{ss'}^{a} = \mathbb{P}\left[S_{t+} = s' \vert S_t = s, A_t = a\right].\]

<p>Therefore, the probability of ending up on all the possible different states depends on the action that you take at the instant \(t\) and at which state you are at time \(t\). The reward function \(\mathcal{R}\) might as well depend on the action:</p>

\[\mathcal{R}^{a}_{s} = \mathbb{E}\left[R_{t+} \vert S_t=s, A_t=a \right].\]

<p>Apart from that, everything is the same.</p>

<h2 id="the-policy">The Policy</h2>

<p>We are now well equipped to define what it means to make and take decisions. In order to do that, we need to define what is called a <strong>policy</strong>. The formal definition of a policy \(\pi\) is a distribution over actions given states:</p>

\[\pi(a\vert s) = \mathbb{P} \left[A_t = a \vert S_t =s \right].\]

<p>In other words, if our system is in a particular state \(S_t\), what’s the
probability of taking a particular action \(A_t\). Therefore, once you have a policy you have fully defined the behavior of an agent taking action in a particular system (remember the chess player? That’s our agent in that example, i.e. the person taking actions).</p>

<p>Another interesting implication of the Markov property is that the policy only depends on the current state (and not the past states) as we discussed for the Chess example. Therefore, the policy is said to be stationary or time-independent.</p>

<p>Also the policy depends on the rewards through the state where the system is, because the state where the system is characterized by the immediate and expected future rewards of the agent.</p>

<p>What defines a Markov reward process given by a chain of states and rewards
is the averaging over policies of our transition probability matrix and reward function, i.e.</p>

\[\begin{align*}
\mathcal{P}^{\pi}_{ss'} &amp;= \sum_{a\in \mathcal{A}} \pi(a \vert s) \mathcal{P}^{a}_{ss'} \\
\mathcal{R}^{\pi}_s &amp;= \sum_{a\in \mathcal{A}} \pi(a \vert s) \mathcal{R}^{a}_{s}
\end{align*}\]

<p>As such the Markov reward process corresponds to the tuple \(\langle \mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi} \rangle\).</p>

<p>Previously, the value function didn’t have any agent, no way to define actions.We have now a way to choose the value function through the policy \(\pi\) and as such, the <em>state-value function</em> \(v_{\pi}(s)\) of an MDP becomes the expected return from state \(s\) that follows a policy \(\pi\)</p>

\[v_{\color{red}{\pi}}(s) =\mathbb{E}_{\color{red}{\pi}}\left[G_t \mid S_t = s \right];\]

<p>we have an expectation \(\mathbb{E}_{\pi}\) over the total return when we sample the actions following the policy \(\pi\).</p>

<p>We can also define a second type of function that is the action value function, which tells us how good it is to take a particular action from a particular state. This is the mathematical object that we should consider when we have to take a particular action. Therefore, the action value function is the expected return from a state \(s\), taking an action \(a\) and by following a particular policy \(\pi\):</p>

\[q_{\pi}(a,s) = \mathbb{E}_{\pi}\left[G_t \vert S_t = s, A_t = a \right]\]

<p>A new Bellman equation is obtained, as previously, by decomposing the immediate reward plus the value of the next state:</p>

\[v_{\pi}(s) = \mathbb{E}_{\pi}\left[R_{t}+\gamma v_{\pi}(S_{t+1}) \vert S_t = s \right].\]

<p>In a similar way, we can get an equation for the action-value function</p>

\[q_{\pi}(s,a) = \mathbb{E}_{\pi} \left[R_t+\gamma q_{\pi}(S_{t+1},A_{t+1}) \vert S_t =s \right].\]

<p>That last equation allows to relate the action-value of the next state with respect to the state where my system is right now.</p>

<p>The way to underst it a bit better is through the relationship that is present between \(v\) and \(q\). So, in order to get \(v_{\pi}(s)\), we are actually averaging over all the possible actions that we might take in the future. Since each action is really <em>valued</em> by the action-value function \(q_{\pi}\) (at the next state), then we need to average over all the action-values under a certain policy \(\pi\) (since the actions are actually sampled from a particular policy), providing us the value of the present state \(s\), i.e.</p>

\[v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \vert s) q_{\pi}(s,a).\]

<p>Let’s now consider instead that  we are going to take a particular action \(a\) at a particular state \(s\). If we take the example of the chess game, we are not asking the question: how good is it to take a specific move, while in the previous paragraph, we were asking how good is it to be where I am now in the game (basically, my probability of winning the game). Therefore, we have to average over the possible states where the action that we are taking are going to lead me (plus the immediate reward), i.e.</p>

\[q_{\pi}(s,a) = \mathcal{R}^a_{s}+ \gamma \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}v_{\pi}(s').\]

<p>If we put the last two equations together, we end up with the following recursive relation</p>

\[v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \vert s) \left( \mathcal{R}^a_{s}+ \gamma \sum_{s' \in \mathcal{S}}\mathcal{P}^a_{ss'}v_{\pi}(s')\right)\]

<p>which is the new Bellman equation for \(v_{\pi}\) that we were looking for. We can also do the same trick with \(q_{\pi}\) to end up with the following recursive relation:</p>

\[q_{\pi}(s) = \mathcal{R}^a_{s} + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}^a_{ss'} \sum_{a' \in \mathcal{A}} \pi(a' \vert s') q_{\pi}(s', a').\]

<p>The two equations are actually how we solve the MDPs. If you abstract the math, you understand that the idea behind those two equations are really simple: the value function at the actual step is just the immediate reward plus the value function at the step where you are after taking a particular action.</p>

<p>That’s all fine, but what we are looking for the optimal actions to pick. For that, we will need to get the optimal policy.</p>

<h2 id="the-optimal-policy">The Optimal Policy</h2>

<p>Given a state you are in, you want to pick actions that will provide you the maximum future rewards. That policy is called the optimal policy.</p>

<p>Let’s first defined the <em>optimal state-value</em> function \(v_{*}(s)\) as being the maximum value function over all possible policies:</p>

\[v_{*}(s) = \max_{\pi} v_{\pi}(s),\]

<p>This function basically tells us what’s the maximum possible reward we can extract from the particular system we are in. In a similar fashion, one can define the <em>optimal action-value function</em> \(q_{*}(s,a)\) as being the maximum action-value function over all policies</p>

\[q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a),\]

<p>meaning if you commit to a particular action, what’s the most reward you can get. What’s important to notice is that if we know \(q_{*}\), then we have solved the MDP, because under all policies it allows to understand the maximum reward you can get for a particular action. Therefore, knowing \(q_{*}\) allows us to behave optimally under the MDP. As such, (and again) solving an MDP is actually getting \(q_*\).</p>

<h2 id="bellman-optimality-equation">Bellman Optimality Equation</h2>

<p>Of course, that’s all good. But in practice, how do we get \(q_{*}\) anyways ? Well, you need to actually get the <strong>Bellman optimality equation</strong> and solve it. Before, we were looking at expectation on action and rewards. Now, we are really looking at the maximum returns. So, our two previous equations become:</p>

\[v_{*}(s) = \max_{a}\mathcal{R}^a_{s} + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}^{a}_{ss'}v_{*}(s'),\]

\[q_{*}(s,a) = \mathcal{R}^a_{s} + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}^a_{ss'} \max_{a'}q_{*}(s', a'),\]

<p>based on the fact that an optimal policy is found by maximizing over \(q_{*}(s,a)\), i.e.</p>

\[\pi_{*}(a\mid s) = \begin{cases} 1 &amp; \text{if } a= \text{arg}\max_{a\in\mathcal{A}}q_{}(s,a), \\ 0 &amp; \text{otherwise} . \end{cases}\]

<p>Now that we have the Bellman optimality equation, we should be done. Unfortunately, things are not that easy because in the last two equations \(q_{*}\) and \(v_{*}\) are not linear anymore :-( . Moreover, there is no closed form solution because there are some <strong>max</strong> involved that complexifies the problem. So, we have to resort to iterative solution methods such as Q-learning and dynamic programming methods. We will talk about those in a subsequent blog post.</p>

<h2 id="conclusion">Conclusion</h2>

<p>So, let’s stop here. We have put in place the foundations for the understanding of a reinforcement learning setup through the study of MDPs. We have defined the Markov property and the closely related process of Markov Reward Processes. Then, after having introduced rewards, we talked about policies and the actions that are sampled from those policies. We derived some (Bellman) equations that allow to connect the (action)-value function at a certain state with the (action)-value functions at a future state. Finally, we approached the subject of optimal policies and how to choose optimal actions by solving the Bellman optimality equation.</p>

            </div>

            <!-- Rating -->
            
            <div class="rating mb-4 d-flex align-items-center">
                <strong class="mr-1">Rating:</strong> <div class="rating-holder">
<div class="c-rating c-rating--regular" data-rating-value="3.5">
  <button>1</button>
  <button>2</button>
  <button>3</button>
  <button>4</button>
  <button>5</button>
</div>
</div>
            </div>
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2024-08-13">13 Aug 2024</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#machine-learning">machine learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#reinforcement-learning">reinforcement learning</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://labfab.io//variational-autoencoders/"> &laquo; Latent Variables & Generative Models</a>
            
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'labfab'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

<script type="application/ld+json">
{
  "@context": "http://schema.org/",
  "@type": "Review",
  "itemReviewed": {
    "@type": "Thing",
    "name": "Markov Decision Processes"
  },
  "author": {
    "@type": "Person",
    "name": "StackNets"
  },
  "datePublished": "2024-08-13",
  "reviewRating": {
    "@type": "Rating",
    "ratingValue": "3.5",
    "bestRating": "5"
  }
}
</script>

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt="LabFab" width="75" height="75"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://labfab.us17.list-manage.com/subscribe/post?u=60c617b562edb784e26cfc17e&amp;id=dd31e570e3&amp;f_id=0094c2e1f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank">
        <div class="mc-field-group">
            <label for="mce-EMAIL"></label><input type="email" name="EMAIL" class="required email" id="mce-EMAIL" required="" value="">
            <input type="submit" name="subscribe" id="mc-embedded-subscribe" class="button" value="Subscribe">
        </div>
            <div id="mce-responses" class="clear foot">
                <div class="response" id="mce-error-response" style="display: none;"></div>
                <div class="response" id="mce-success-response" style="display: none;"></div>
            </div>
        <div style="position: absolute; left: -5000px;" aria-hidden="true">
            /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */
            <input type="text" name="b_60c617b562edb784e26cfc17e_dd31e570e3" tabindex="-1" value="">
        </div>
        </div>
      </form>
      </div>
      <script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></div>
      </div>

    


<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#deep-learning">deep learning (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#artificial-intelligence">artificial intelligence (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#unsupervised-learning">unsupervised learning (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#machine-learning">machine learning (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#mathematics">mathematics (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#reinforcement-learning">reinforcement learning (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2024 LabFab 
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//labfab.disqus.com/count.js"></script>


</body>
</html>
