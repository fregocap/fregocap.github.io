<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://labfab.io/assets/images/logo.png">

<title>An Intro to Gaussian Mixture Models | LabFab</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>An Intro to Gaussian Mixture Models | LabFab</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="An Intro to Gaussian Mixture Models" />
<meta name="author" content="stacknets" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Gaussian Mixture Models (GMMs) are powerful unsupervised learning tools used to model multimodal data distributions through a combination of multiple Gaussian distributions, applicable in various fields such as image segmentation, clustering, and audio feature extraction, leveraging the Expectation-Maximization (EM) algorithm for parameter estimation." />
<meta property="og:description" content="Gaussian Mixture Models (GMMs) are powerful unsupervised learning tools used to model multimodal data distributions through a combination of multiple Gaussian distributions, applicable in various fields such as image segmentation, clustering, and audio feature extraction, leveraging the Expectation-Maximization (EM) algorithm for parameter estimation." />
<link rel="canonical" href="http://localhost:4000/https://labfab.io/gaussian-processes/" />
<meta property="og:url" content="http://localhost:4000/https://labfab.io/gaussian-processes/" />
<meta property="og:site_name" content="LabFab" />
<meta property="og:image" content="http://localhost:4000/https://labfab.io/assets/images/main_gmm.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-01T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/https://labfab.io/assets/images/main_gmm.png" />
<meta property="twitter:title" content="An Intro to Gaussian Mixture Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"stacknets"},"dateModified":"2024-05-01T00:00:00+02:00","datePublished":"2024-05-01T00:00:00+02:00","description":"Gaussian Mixture Models (GMMs) are powerful unsupervised learning tools used to model multimodal data distributions through a combination of multiple Gaussian distributions, applicable in various fields such as image segmentation, clustering, and audio feature extraction, leveraging the Expectation-Maximization (EM) algorithm for parameter estimation.","headline":"An Intro to Gaussian Mixture Models","image":"http://localhost:4000/https://labfab.io/assets/images/main_gmm.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/https://labfab.io/gaussian-processes/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/https://labfab.io/assets/images/logo.png"},"name":"stacknets"},"url":"http://localhost:4000/https://labfab.io/gaussian-processes/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://labfab.io/assets/css/screen.css" rel="stylesheet">

<link href="https://labfab.io/assets/css/main.css" rel="stylesheet">

<script src="https://labfab.io/assets/js/jquery.min.js"></script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://labfab.io/">
    <img src="https://labfab.io/assets/images/logo.png" alt="LabFab">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://labfab.io/index.html">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://labfab.io/about">About</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li> -->


                <script src="https://labfab.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://labfab.io/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">LabFab</h1>
    <p class="lead">
        Exploring math, physics, machine learning, and finance insights.
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=An Intro to Gaussian Mixture Models&url=http://localhost:4000/https://labfab.io/gaussian-processes/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/https://labfab.io/gaussian-processes/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/https://labfab.io/gaussian-processes/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://labfab.io/assets/images/avatar.png" alt="StackNets">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://labfab.io">StackNets</a><a target="_blank" href="https://twitter.com/capelfabio" class="btn follow">Follow</a>
                        <span class="author-description">I'm interested in machine learning, trading and running. I'm currently learning how to make the best focaccia possible and looking to collaborate on any project that makes me grow (though perhaps not in that order).</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">An Intro to Gaussian Mixture Models</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid" src="https://labfab.io/assets/images/main_gmm.png" alt="An Intro to Gaussian Mixture Models">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#introduction-to-gmms">Introduction to GMMs</a>
    <ul>
      <li><a href="#example-of-gmm">Example of GMM</a></li>
    </ul>
  </li>
  <li><a href="#application-of-gmms">Application of GMMs</a></li>
  <li><a href="#mathematical-formulation-of-gmms">Mathematical Formulation of GMMs</a></li>
  <li><a href="#training-the-gmm-expectation-maximization-algorithm">Training the GMM: Expectation-Maximization algorithm</a>
    <ul>
      <li><a href="#steps-of-the-em-algorithm">Steps of the EM Algorithm</a></li>
    </ul>
  </li>
  <li><a href="#clustering-with-gmms">Clustering with GMMs</a></li>
  <li><a href="#challenges-and-considerations">Challenges and Considerations</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p>In this blog post, we will explore the concept of <strong>Gaussian Mixture Models</strong> (GMMs). These models are intuitive and widely applicable in various domains such as image segmentation, clustering, and generative modeling.</p>

<h2 id="introduction-to-gmms">Introduction to GMMs</h2>

<p>Gaussian Mixture Models are used to model an overall distribution through multiple Gaussian distributions. They are a powerful tool for capturing, estimating, and clustering parts of an overall distribution as locally Gaussian-distributed. GMMs are unsupervised models, meaning they do not need to know the specific Gaussian distribution a data point belongs to in advance.</p>

<h3 id="example-of-gmm">Example of GMM</h3>

<p>Here‚Äôs a simple example to illustrate GMMs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># first gaussian
</span><span class="n">mean1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">standard_deviation</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># second gaussian
</span><span class="n">mean2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.5</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">mean2</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># third gaussian
</span><span class="n">mean3</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">mean3</span><span class="p">,</span> <span class="n">standard_deviation</span><span class="p">)</span>

<span class="c1"># overall plotting
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">y2</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">y3</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Coordinates</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Density</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="https://labfab.io/assets/images/density_estimation.png" alt="'Density Estimation With Gaussian Process'" /></p>

<p>This example shows three Gaussian distributions fitting an overall distribution, with the black line representing the combined distribution.</p>

<h2 id="application-of-gmms">Application of GMMs</h2>

<p>GMMs have numerous applications, including:</p>

<ul>
  <li>Image segmentation</li>
  <li>Multi-object tracking in videos</li>
  <li>Audio feature extraction</li>
</ul>

<p>They are particularly useful for multimodal distributions, where multiple peaks are present. These peaks can be modeled using multiple Gaussian distributions.</p>

<h2 id="mathematical-formulation-of-gmms">Mathematical Formulation of GMMs</h2>

<p>To represent GMMs mathematically, we need to understand three types of parameters:</p>

<ol>
  <li><strong>Mixture Weights</strong> (\(\phi\)): indicate the probability that a point belongs to a specific Gaussian component \(K\).</li>
  <li><strong>Means</strong> (\(\mu\)): the centers of each Gaussian component.</li>
  <li><strong>Covariances</strong> (\(\mathbf{\Sigma}_i\)): describe the spread and orientation of each Gaussian component.</li>
</ol>

<p>The probability density function for a GMM is given by:</p>

\[p(\mathbf{x}) = \sum_{i=1}^K \phi_i(\mathbf{x}) \mathcal{N}(\mathbf{x}|\mathbf{\mu}_i, \mathbf{\Sigma}_i)\]

<p>where \(\mathcal{N}(\mathbf{x}\vert\mathbf{\mu}_i, \mathbf{\Sigma}_i)\) is the multivariate Gaussian distribution.</p>

<h2 id="training-the-gmm-expectation-maximization-algorithm">Training the GMM: Expectation-Maximization algorithm</h2>

<p>The EM algorithm is used to find the maximum likelihood parameters of a GMM, especially when there are latent variables influencing the data distribution.</p>

<h3 id="steps-of-the-em-algorithm">Steps of the EM Algorithm</h3>

<ol>
  <li><strong>Expectation (E-step)</strong>: Estimate the latent variables.</li>
  <li><strong>Maximization (M-step)</strong>: Maximize the parameters based on the current estimates of the latent variables.</li>
</ol>

<p>In the particular case of GMMs, if one considers the maximum likelihood, we should maximize:</p>

\[\ln p(\mathbf{x}|\phi_i, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^N \ln\left( \sum_{i=1}^K \phi_i(\mathbf{x}^{(n)}) \mathcal{N}(\mathbf{x}^{(n)}|\mathbf{\mu}_i, \mathbf{\Sigma}_i) \right)\]

<p>with respect to \(\theta = (\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i)\). However,
two problems arise by doing so: 1) we can have very high (arbitrarily large) likelihood when a
single Gaussian explains a point; 2) an unlimited number of solutions is acceptable up to
permutations. Instead, if we do introduce a latent variable \(z\), then one can consider that the
mixture model generates the data by first sampling from \(z\), and only then we sample the
observable data \(\mathbf{x}\) from a distribution that does depend on \(z\), meaning:</p>

\[p(z,\mathbf{x}) = p(z)p(\mathbf{x}|z).\]

<p>In mixture models, the <strong>latent variables are easily interpreted as being the different
components of the data distribution</strong>, i.e. \(z=c\) .</p>

<p>Let us try to optimize \(\ln p(\mathbf{x})\) for the set of parameters \(\theta\) by integrating
over the latent variable</p>

\[\begin{align}
\frac{d}{d\theta} \ln p(\mathbf{x}) &amp; = \frac{d}{d\theta} \ln \sum_{z} p(z, \mathbf{x})  \\
&amp; =  \frac{\frac{d}{d\theta} \sum_{z} p(z,\mathbf{x})}{\sum_{z'}p(z',\mathbf{x})}\\
&amp; = \sum_{z} p(z|\mathbf{x}) \frac{d}{d\theta} \ln p(z,\mathbf{x}) \\
&amp; = \mathbb{E}_{p(z|\mathbf{x})} \left[\frac{d}{d\theta} \ln p(z,\mathbf{x}) \right]. 
\end{align}\]

<p>This means that the derivative of the marginal log-probability \(p(\mathbf{x})\) is the expected
value of the derivative of the joint log-probability, with the expectation on the posterior
distribution. This formula is completely generic for any model with latent variables as we
did not introduce any specificities related to GMMs. We have not given the full details of
the derivation, but just keep in mind that we have used the known property:</p>

\[\frac{d}{d\theta} \ln A(\theta) = \frac{1}{A(\theta)} \frac{d}{d\theta} A(\theta).\]

<p>It is rather tempting to equalize the derivative to zero for the particular case of the GMMs.
Doing so, you end up with the optimum parameters that we are looking for. In particular, our
two previous steps of the EM algorithm become:</p>

<p><strong>E-Step</strong>:</p>

\[r_{ni} :=p(z_{n} = i | \mathbf{x}_n) = \frac{\phi_i \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_i,\mathbf{\Sigma}_i)}{\sum_{j=1}^K \phi_j \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_j,\mathbf{\Sigma}_j)}\]

<p><strong>M-Step</strong>:</p>

\[\begin{align}
\phi_i &amp; = \frac{\sum_{n=1}^N r_{ni}}{\sum_{i=1}^K \sum_{n=1}^N r_{ni}}, \\
\mathbf{\mu}_i &amp; = \frac{\sum_{n=1}^N  r_{ni}\mathbf{x}_n}{\sum_{n=1}^N r_{ni}}, \\
\Sigma_{i} &amp; = \frac{\sum_{n=1}^N r_{ni} \left(\mathbf{x}_n -\mathbf{\mu}_i\right)\left(\mathbf{x}_n - \mathbf{\mu}_i\right)^\intercal}{\sum_{n=1}^N r_{ni}}
\end{align}\]

<p>These two steps define fully the EM algorithm for the GMMs with a random initialization of
the parameters \(\theta = \left\{\phi_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i\right\}\).</p>

<h2 id="clustering-with-gmms">Clustering with GMMs</h2>

<p>To use GMMs for clustering, follow these steps:</p>

<ol>
  <li>Train the model to obtain the parameters (means, covariances).</li>
  <li>Assign each data point to a Gaussian component based on the probability \(r_{ni}\).</li>
</ol>

<p>Let us first start by generating two non-trivial distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">]]</span>
<span class="n">cov2</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">]]</span>

<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">5000</span><span class="p">).</span><span class="n">T</span>
<span class="n">x2</span><span class="p">,</span><span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="mi">5000</span><span class="p">).</span><span class="n">T</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="https://labfab.io/assets/images/gmm_clustering1.png" alt="noise" /></p>

<p>As you can see for the second example, we did not consider a diagonal covariance matrix.
That‚Äôs also an interesting case, because there is some overlapping region that will definitely
be challenging for the algorithm to understand.</p>

<p>The stopping criteria is related to the difference between the log-likelihood at a step \(ùëõ-1\)
and step \(ùëõ\)  being below a certain threshold. Meanwhile, until that threshold is not reached
we continue updating the estimated parameters of the model. Let‚Äôs therefore built a class with
a <code class="language-plaintext highlighter-rouge">fit</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">GMM</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="n">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="n">n_iters</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> 
        <span class="sh">"""</span><span class="s"> Method that learns the parameters of the GMM
        </span><span class="sh">"""</span>
        <span class="c1"># initialization of the parameters
</span>        <span class="n">old_log_likelihood</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1">## rni and weights, i.e. prior
</span>        <span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rni</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_row</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">)</span>
            
        <span class="c1"># mean initialization
</span>        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span>

        <span class="c1"># covariance matrix initialization
</span>        <span class="n">shape_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_col</span><span class="p">,</span> <span class="n">n_col</span>
        <span class="n">self</span><span class="p">.</span><span class="n">covariances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span><span class="n">shape_var</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
        
        <span class="c1"># start the main loop
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_iters</span><span class="p">):</span>
            <span class="c1"># compute first the log-likelihhod
</span>            <span class="n">self</span><span class="p">.</span><span class="nf">__log_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">new_log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
        
            <span class="c1"># run the E-M step
</span>            <span class="n">self</span><span class="p">.</span><span class="nf">__E_step</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">__M_step</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            
            <span class="c1"># check convergence 
</span>            <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">new_log_likelihood</span> <span class="o">-</span> <span class="n">old_log_likelihood</span><span class="p">)</span> <span class="o">&lt;=</span><span class="n">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="c1"># if it did not converge, then update the log-likelihood
</span>            <span class="n">old_log_likelihood</span> <span class="o">=</span> <span class="n">new_log_likelihood</span>
                        
        
    
    <span class="k">def</span> <span class="nf">__E_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Method that implements the E-step
        </span><span class="sh">"""</span>
        <span class="c1">#normalize over the different cluster probabilities
</span>        <span class="n">self</span><span class="p">.</span><span class="n">rni</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">__M_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Method that implements the M-step
        </span><span class="sh">"""</span>
        <span class="n">phi_num</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">phi_i</span> <span class="o">=</span> <span class="n">phi_num</span> <span class="o">/</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># means
</span>        <span class="n">self</span><span class="p">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">/</span> <span class="n">phi_num</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">#covariances
</span>        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">):</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">]).</span><span class="n">T</span>
            <span class="n">cov_num</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">covariances</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">cov_num</span> <span class="o">/</span> <span class="n">phi_num</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        
    
    <span class="k">def</span> <span class="nf">__log_likelihood</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Method to get the log-likelihood
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">):</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="n">likelihood</span> <span class="o">=</span> <span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">covariances</span><span class="p">[</span><span class="n">k</span><span class="p">]).</span><span class="nf">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">rni</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>
            
    
    <span class="k">def</span> <span class="nf">plot_component</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Clusters</span><span class="sh">'</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> Method that plots the different components assigned to the data points X
        </span><span class="sh">"""</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        
        <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.25</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="n">delta</span><span class="p">)</span>
        <span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">coordinates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x_grid</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_grid</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()]).</span><span class="n">T</span> 
        
        <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_components</span><span class="p">):</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">covariances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">z_grid</span> <span class="o">=</span> <span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">).</span><span class="nf">pdf</span><span class="p">(</span><span class="n">coordinates</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">col</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            
        <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p>One can apply such class to our previous dataset and follows the figure below:</p>

<p><img src="https://labfab.io/assets/images/gmm_components.png" alt="'Density Estimation With Gaussian Process'" /></p>

<h2 id="challenges-and-considerations">Challenges and Considerations</h2>

<p>GMMs require specifying the number of components beforehand. Model selection criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can help determine the optimal number of components. Additionally, the complexity of GMMs increases with the size of the dataset, particularly due to the covariance matrices. Simplifying assumptions, such as diagonal covariance matrices, can mitigate this issue.</p>

<p>In summary, Gaussian Mixture Models are a robust tool for modeling and clustering complex data distributions. Their ability to handle multimodal distributions makes them valuable in various practical applications.</p>

            </div>

            <!-- Rating -->
            
            <div class="rating mb-4 d-flex align-items-center">
                <strong class="mr-1">Rating:</strong> <div class="rating-holder">
<div class="c-rating c-rating--regular" data-rating-value="3.5">
  <button>1</button>
  <button>2</button>
  <button>3</button>
  <button>4</button>
  <button>5</button>
</div>
</div>
            </div>
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2024-05-01">01 May 2024</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://labfab.io/categories#machine-learning">machine learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://labfab.io/categories#unsupervised-learning">unsupervised learning</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://labfab.io//diffusion-models/"> &laquo; Unlocking Creativity: My Journey into DALL-E 2 & Diffusion Models</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="https://labfab.io//cauchy-schwarz/">A Simple Proof of the Cauchy-Schwarz Inequality &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'labfab'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

<script type="application/ld+json">
{
  "@context": "http://schema.org/",
  "@type": "Review",
  "itemReviewed": {
    "@type": "Thing",
    "name": "An Intro to Gaussian Mixture Models"
  },
  "author": {
    "@type": "Person",
    "name": "StackNets"
  },
  "datePublished": "2024-05-01",
  "reviewRating": {
    "@type": "Rating",
    "ratingValue": "3.5",
    "bestRating": "5"
  }
}
</script>


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3K2EDM9K7H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3K2EDM9K7H');
</script>
</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://labfab.io/assets/images/logo.png" alt="LabFab" width="75" height="75"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://labfab.us17.list-manage.com/subscribe/post?u=60c617b562edb784e26cfc17e&amp;id=dd31e570e3&amp;f_id=0094c2e1f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank">
        <div class="mc-field-group">
            <label for="mce-EMAIL"></label><input type="email" name="EMAIL" class="required email" id="mce-EMAIL" required="" value="">
            <input type="submit" name="subscribe" id="mc-embedded-subscribe" class="button" value="Subscribe">
        </div>
            <div id="mce-responses" class="clear foot">
                <div class="response" id="mce-error-response" style="display: none;"></div>
                <div class="response" id="mce-success-response" style="display: none;"></div>
            </div>
        <div style="position: absolute; left: -5000px;" aria-hidden="true">
            /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */
            <input type="text" name="b_60c617b562edb784e26cfc17e_dd31e570e3" tabindex="-1" value="">
        </div>
        </div>
      </form>
      </div>
      <script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></div>
      </div>

    


<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">‚Üí</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#deep-learning">deep learning (1)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#artificial-intelligence">artificial intelligence (1)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#generative-ai">generative ai (1)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#diffusion-models">diffusion models (1)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#unsupervised-learning">unsupervised learning (2)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#machine-learning">machine learning (3)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#mathematics">mathematics (1)</a>
                
                    <a class="mt-1 mb-1" href="https://labfab.io/categories#reinforcement-learning">reinforcement learning (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright ¬© 2025 LabFab 
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://labfab.io/assets/js/mediumish.js"></script>



<script src="https://labfab.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//labfab.disqus.com/count.js"></script>


</body>
</html>
